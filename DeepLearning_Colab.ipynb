{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
        "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
        "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
        "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
        "Remember to search for the right learning rate each time you change the model’s\n",
        "architecture or hyperparameters.**"
      ],
      "metadata": {
        "id": "LETlLSTsclat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Nadam optimizer and categorical cross-entropy loss\n",
        "model.compile(optimizer=Nadam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback with a patience of 5 epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# Train the model for 100 epochs with batch size of 128 and early stopping\n",
        "history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mjngEJCbrRv",
        "outputId": "13cca819-bc7d-4d9c-e9a7-454313c26c5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 6s 9ms/step - loss: 1.6740 - accuracy: 0.3795 - val_loss: 1.3469 - val_accuracy: 0.5176\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3115 - accuracy: 0.5302 - val_loss: 1.1754 - val_accuracy: 0.5880\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.1712 - accuracy: 0.5841 - val_loss: 1.0493 - val_accuracy: 0.6352\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 1.0844 - accuracy: 0.6174 - val_loss: 1.0307 - val_accuracy: 0.6345\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.0254 - accuracy: 0.6375 - val_loss: 0.9420 - val_accuracy: 0.6756\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.9731 - accuracy: 0.6580 - val_loss: 0.9196 - val_accuracy: 0.6779\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9276 - accuracy: 0.6736 - val_loss: 0.8598 - val_accuracy: 0.6988\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.8901 - accuracy: 0.6866 - val_loss: 0.8505 - val_accuracy: 0.7028\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.8560 - accuracy: 0.6988 - val_loss: 0.8293 - val_accuracy: 0.7129\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.8265 - accuracy: 0.7084 - val_loss: 0.7915 - val_accuracy: 0.7236\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.8011 - accuracy: 0.7179 - val_loss: 0.7723 - val_accuracy: 0.7303\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.7763 - accuracy: 0.7267 - val_loss: 0.7587 - val_accuracy: 0.7339\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.7551 - accuracy: 0.7345 - val_loss: 0.7728 - val_accuracy: 0.7305\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.7402 - accuracy: 0.7400 - val_loss: 0.7463 - val_accuracy: 0.7400\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.7195 - accuracy: 0.7454 - val_loss: 0.7327 - val_accuracy: 0.7418\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.7067 - accuracy: 0.7528 - val_loss: 0.7223 - val_accuracy: 0.7484\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.6946 - accuracy: 0.7565 - val_loss: 0.7064 - val_accuracy: 0.7568\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.6763 - accuracy: 0.7614 - val_loss: 0.7263 - val_accuracy: 0.7465\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.6668 - accuracy: 0.7655 - val_loss: 0.7527 - val_accuracy: 0.7409\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.6496 - accuracy: 0.7699 - val_loss: 0.7544 - val_accuracy: 0.7432\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.6380 - accuracy: 0.7765 - val_loss: 0.7029 - val_accuracy: 0.7564\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.6265 - accuracy: 0.7780 - val_loss: 0.7141 - val_accuracy: 0.7507\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.6209 - accuracy: 0.7785 - val_loss: 0.6923 - val_accuracy: 0.7623\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.6046 - accuracy: 0.7873 - val_loss: 0.7110 - val_accuracy: 0.7571\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5991 - accuracy: 0.7885 - val_loss: 0.6758 - val_accuracy: 0.7696\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5894 - accuracy: 0.7907 - val_loss: 0.6778 - val_accuracy: 0.7665\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5737 - accuracy: 0.7957 - val_loss: 0.6797 - val_accuracy: 0.7653\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.5679 - accuracy: 0.7969 - val_loss: 0.6733 - val_accuracy: 0.7669\n",
            "Epoch 29/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5624 - accuracy: 0.7993 - val_loss: 0.7049 - val_accuracy: 0.7605\n",
            "Epoch 30/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5540 - accuracy: 0.8039 - val_loss: 0.6713 - val_accuracy: 0.7740\n",
            "Epoch 31/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5453 - accuracy: 0.8054 - val_loss: 0.6843 - val_accuracy: 0.7682\n",
            "Epoch 32/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.5397 - accuracy: 0.8071 - val_loss: 0.6939 - val_accuracy: 0.7645\n",
            "Epoch 33/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5379 - accuracy: 0.8072 - val_loss: 0.6884 - val_accuracy: 0.7665\n",
            "Epoch 34/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5303 - accuracy: 0.8113 - val_loss: 0.6681 - val_accuracy: 0.7689\n",
            "Epoch 35/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5201 - accuracy: 0.8137 - val_loss: 0.7065 - val_accuracy: 0.7559\n",
            "Epoch 36/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.5158 - accuracy: 0.8158 - val_loss: 0.6921 - val_accuracy: 0.7708\n",
            "Epoch 37/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5063 - accuracy: 0.8188 - val_loss: 0.6912 - val_accuracy: 0.7686\n",
            "Epoch 38/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.5050 - accuracy: 0.8182 - val_loss: 0.6668 - val_accuracy: 0.7691\n",
            "Epoch 39/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.4956 - accuracy: 0.8208 - val_loss: 0.6813 - val_accuracy: 0.7673\n",
            "Epoch 40/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.4924 - accuracy: 0.8232 - val_loss: 0.6769 - val_accuracy: 0.7712\n",
            "Epoch 41/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.4872 - accuracy: 0.8264 - val_loss: 0.6844 - val_accuracy: 0.7756\n",
            "Epoch 42/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.4849 - accuracy: 0.8279 - val_loss: 0.6771 - val_accuracy: 0.7719\n",
            "Epoch 43/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 0.4748 - accuracy: 0.8300 - val_loss: 0.7206 - val_accuracy: 0.7637\n",
            "Epoch 43: early stopping\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.7206 - accuracy: 0.7637\n",
            "Test accuracy: 0.763700008392334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now try adding Batch Normalization and compare the learning curves: Is it\n",
        "converging faster than before? Does it produce a better model? How does it affect\n",
        "training speed?**"
      ],
      "metadata": {
        "id": "HB81uA_ic-lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import lecun_normal\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model architecture with SELU activation and LeCun initialization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='selu', input_shape=(32,32,3), kernel_initializer=lecun_normal()),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal()),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal()),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Nadam optimizer and categorical cross-entropy loss\n",
        "model.compile(optimizer=Nadam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback with a patience of 5 epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# Train the model for 100 epochs with batch size of 128 and early stopping\n",
        "history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsWqkILPkcpS",
        "outputId": "09780cc6-514e-47d3-9bdb-dff9f4ffd23f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 7s 10ms/step - loss: 1.5884 - accuracy: 0.4363 - val_loss: 1.2757 - val_accuracy: 0.5485\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 1.2705 - accuracy: 0.5513 - val_loss: 1.2012 - val_accuracy: 0.5763\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.1710 - accuracy: 0.5866 - val_loss: 1.0914 - val_accuracy: 0.6167\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 1.0966 - accuracy: 0.6141 - val_loss: 1.0598 - val_accuracy: 0.6287\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0442 - accuracy: 0.6322 - val_loss: 1.0008 - val_accuracy: 0.6536\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0005 - accuracy: 0.6467 - val_loss: 0.9665 - val_accuracy: 0.6693\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.9643 - accuracy: 0.6614 - val_loss: 0.9336 - val_accuracy: 0.6760\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9260 - accuracy: 0.6755 - val_loss: 0.9163 - val_accuracy: 0.6832\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.8948 - accuracy: 0.6834 - val_loss: 0.9060 - val_accuracy: 0.6861\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8696 - accuracy: 0.6947 - val_loss: 0.9048 - val_accuracy: 0.6902\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.8478 - accuracy: 0.7039 - val_loss: 0.8851 - val_accuracy: 0.6970\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.8264 - accuracy: 0.7111 - val_loss: 0.8850 - val_accuracy: 0.6941\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.8074 - accuracy: 0.7154 - val_loss: 0.9026 - val_accuracy: 0.6897\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.7934 - accuracy: 0.7208 - val_loss: 0.8427 - val_accuracy: 0.7101\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.7787 - accuracy: 0.7256 - val_loss: 0.8206 - val_accuracy: 0.7170\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.7680 - accuracy: 0.7291 - val_loss: 0.8603 - val_accuracy: 0.7064\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.7519 - accuracy: 0.7366 - val_loss: 0.8370 - val_accuracy: 0.7089\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.7395 - accuracy: 0.7398 - val_loss: 0.8242 - val_accuracy: 0.7136\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.7287 - accuracy: 0.7438 - val_loss: 0.8328 - val_accuracy: 0.7142\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 3s 9ms/step - loss: 0.7209 - accuracy: 0.7445 - val_loss: 0.8671 - val_accuracy: 0.7087\n",
            "Epoch 20: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8671 - accuracy: 0.7087\n",
            "Test accuracy: 0.7087000012397766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model architecture with Batch Normalization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Nadam optimizer and categorical cross-entropy loss\n",
        "model.compile(optimizer=Nadam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback with a patience of 5 epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# Train the model for 100 epochs with batch size of 128 and early stopping\n",
        "history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG63cTzgY4RV",
        "outputId": "5a2dc04c-8518-4cbe-97b1-43aa1a4d2847"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 20s 11ms/step - loss: 1.4824 - accuracy: 0.4744 - val_loss: 2.5087 - val_accuracy: 0.2342\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.0962 - accuracy: 0.6117 - val_loss: 1.2763 - val_accuracy: 0.5586\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.9519 - accuracy: 0.6641 - val_loss: 1.0391 - val_accuracy: 0.6313\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.8678 - accuracy: 0.6924 - val_loss: 1.0474 - val_accuracy: 0.6273\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8102 - accuracy: 0.7132 - val_loss: 0.8746 - val_accuracy: 0.6915\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.7630 - accuracy: 0.7318 - val_loss: 0.8508 - val_accuracy: 0.7028\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.7232 - accuracy: 0.7453 - val_loss: 0.8384 - val_accuracy: 0.7054\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6902 - accuracy: 0.7577 - val_loss: 0.9987 - val_accuracy: 0.6543\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6621 - accuracy: 0.7662 - val_loss: 0.8120 - val_accuracy: 0.7147\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.6402 - accuracy: 0.7742 - val_loss: 1.1103 - val_accuracy: 0.6257\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.6138 - accuracy: 0.7841 - val_loss: 0.8375 - val_accuracy: 0.7084\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5925 - accuracy: 0.7905 - val_loss: 0.6853 - val_accuracy: 0.7616\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.5818 - accuracy: 0.7941 - val_loss: 0.7839 - val_accuracy: 0.7306\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.5656 - accuracy: 0.8011 - val_loss: 0.9965 - val_accuracy: 0.6654\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5519 - accuracy: 0.8047 - val_loss: 0.8183 - val_accuracy: 0.7160\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.5369 - accuracy: 0.8096 - val_loss: 0.7785 - val_accuracy: 0.7322\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 4s 9ms/step - loss: 0.5172 - accuracy: 0.8174 - val_loss: 0.7030 - val_accuracy: 0.7592\n",
            "Epoch 17: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.7592\n",
            "Test accuracy: 0.7591999769210815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training and validation accuracy over the epochs\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss over the epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "Mqy990xqZYsB",
        "outputId": "8a6c1d01-c7a8-4332-e26c-50dae36b67c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4MklEQVR4nO3dd3yV9dn48c+VPciAECAQIIhsFZSIomjFrShoXeCCatX6c9ZWq7b1sbZ96tOnS32sVusAFyouUOoWrRWRIRvCDmRBgCwSsq/fH9+TcAgJBMgZnHO9X6+8cs593+ec6zC+1/3doqoYY4wJXxGBDsAYY0xgWSIwxpgwZ4nAGGPCnCUCY4wJc5YIjDEmzFkiMMaYMGeJwIQFEckSERWRqHZcO0VEvvZHXMYEA0sEJuiIyCYRqRWRri2Of+8pzLMCFJoxIckSgQlWG4FJTU9E5FggIXDhBIf21GiMOViWCEywegm43uv5ZGCa9wUikiIi00SkWERyReRXIhLhORcpIn8Ske0isgEY18prnxORQhHJF5HfiUhkewITkTdFpEhEykTkKxEZ5nUuXkT+7ImnTES+FpF4z7kxIvKNiJSKyBYRmeI5PkdEfuz1Hns1TXlqQbeJyFpgrefYY573KBeRhSJymtf1kSLyoIisF5EKz/neIvKkiPy5xXeZKSI/bc/3NqHLEoEJVt8CySIyxFNATwRebnHNE0AKcBTwA1zi+JHn3E3ARcDxQDZweYvXvgjUA0d7rjkX+DHt8y9gANANWAS84nXuT8BI4BSgC3Af0CgifT2vewJIB0YAi9v5eQCXACcBQz3P53veowvwKvCmiMR5zt2Dq01dCCQDNwBVwFRgkley7Aqc7Xm9CWeqaj/2E1Q/wCZcAfUr4A/A+cAnQBSgQBYQCdQCQ71edwswx/P4c+AnXufO9bw2CugO1ADxXucnAV94Hk8Bvm5nrKme903B3VjtBoa3ct0DwDttvMcc4Mdez/f6fM/7n3mAOEqaPhfIASa0cd0q4BzP49uB2YH++7afwP9Ye6MJZi8BXwH9aNEsBHQFooFcr2O5QC/P457AlhbnmvT1vLZQRJqORbS4vlWe2snvgStwd/aNXvHEAnHA+lZe2ruN4+21V2wi8nPgRtz3VNydf1Pn+v4+aypwLS6xXgs8dhgxmRBhTUMmaKlqLq7T+ELg7RantwN1uEK9SR8g3/O4EFcgep9rsgVXI+iqqqmen2RVHcaBXQ1MwNVYUnC1EwDxxFQN9G/ldVvaOA5Qyd4d4T1auaZ5mWBPf8B9wJVAZ1VNBco8MRzos14GJojIcGAI8G4b15kwYonABLsbcc0ild4HVbUBeAP4vYgkedrg72FPP8IbwJ0ikikinYH7vV5bCHwM/FlEkkUkQkT6i8gP2hFPEi6J7MAV3v/t9b6NwPPAX0Skp6fTdrSIxOL6Ec4WkStFJEpE0kRkhOeli4EfikiCiBzt+c4HiqEeKAaiROQhXI2gyT+B34rIAHGOE5E0T4x5uP6Fl4C3VHV3O76zCXGWCExQU9X1qrqgjdN34O6mNwBf4zo9n/ecexb4CFiC69BtWaO4HogBVuLa12cAGe0IaRqumSnf89pvW5z/ObAMV9juBP4HiFDVzbiazc88xxcDwz2v+Suuv2MrrunmFfbvI+BDYI0nlmr2bjr6Cy4RfgyUA88B8V7npwLH4pKBMYiqbUxjTDgRkdNxNae+agWAwWoExoQVEYkG7gL+aUnANLFEYEyYEJEhQCmuCexvAQ3GBBVrGjLGmDBnNQJjjAlzR9yEsq5du2pWVlagwzDGmCPKwoULt6tqemvnjrhEkJWVxYIFbY0mNMYY0xoRyW3rnDUNGWNMmLNEYIwxYc4SgTHGhLkjro+gNXV1deTl5VFdXR3oUHwuLi6OzMxMoqOjAx2KMSZEhEQiyMvLIykpiaysLLyWFQ45qsqOHTvIy8ujX79+gQ7HGBMiQqJpqLq6mrS0tJBOAgAiQlpaWljUfIwx/hMSiQAI+STQJFy+pzHGf0KiacgYY0KBqlJRU09pZR0lVbWUVNVStruOkspaSqrqOHNwN4b3Tu3wz7VE0AF27NjBWWedBUBRURGRkZGkp7sJfN999x0xMTFtvnbBggVMmzaNxx9/3C+xGmP8o7qugdKqPQV60+PSqjpKq1zBXtry+O46GhrbXv+ta1KsJYJglZaWxuLFiwF4+OGH6dSpEz//+c+bz9fX1xMV1fofdXZ2NtnZ2f4I0xjTQarrGigsq6awbDeFpe53QVk1RWXVFJTuprCsmrLddW2+Pi46gs4JMaTER9M5IYZBPZJITYihc0I0qfExpCa4450To0lNiCE1PpqU+GiiIn3Tmm+JwEemTJlCXFwc33//PaeeeioTJ07krrvuorq6mvj4eF544QUGDRrEnDlz+NOf/sT777/Pww8/zObNm9mwYQObN2/m7rvv5s477wz0VzEmrNTUN7C1rMYV8mXVFDQX9tXNx3ZW1u7zus4J0WSkxJPZOZ7srM70SI6jc2IMnRP2FOxNv+OiIwPwzdrm00QgIucDjwGRuI0wHm1xvg9u27xUzzX3q+rsw/nM38xawcqC8sN5i30M7ZnMf13cnn3N95aXl8c333xDZGQk5eXl/Pvf/yYqKopPP/2UBx98kLfeemuf16xevZovvviCiooKBg0axK233mpzBow5DNV1Da55pnJPk4xritnzuKyqjuJdNRSUVrN9V80+75ESH01GShwZKXEM751KRnIcGanx9Exxv3skxxEfE1yF+8HwWSIQkUjgSeAcIA+YLyIzVXWl12W/At5Q1adEZCgwG8jyVUz+dsUVVxAZ6f5xlJWVMXnyZNauXYuIUFfXerVx3LhxxMbGEhsbS7du3di6dSuZmZn+DNuYoLa7toF123axraK6uZ29xKvNvaSybq82+Zr6xjbfKyEmsvlOPa1TLEMzkslIiScjNc5T8MeTkRJHYmxoN5748tuNAtap6gYAEZkOTMBt+N1EgWTP4xSg4HA/9FDu3H0lMTGx+fGvf/1rxo4dyzvvvMOmTZs444wzWn1NbGxs8+PIyEjq6+t9HaYxQamxUckv3c2qwnJWF1Wwuqic1YUVbNxRScv9tCIjhNT46Oaml8zOCRzbK5rOiV7t7QnRpMS7dvemwj826si9i+9IvkwEvYAtXs/zgJNaXPMw8LGI3AEkAme39kYicjNwM0CfPn06PFB/KCsro1evXgC8+OKLgQ3GmCBTUV1HTlEFq4oqWO0p+HOKKthVs+dGqG9aAoN7JHHx8J4M7pFEz9R4V6AnRpMUG2VzbA5DoOs7k4AXVfXPIjIaeElEjlHVvepyqvoM8AxAdnb2Ebm35n333cfkyZP53e9+x7hx4wIdjjEB0dCobNxeyeqiclfwF7o7/byS3c3XJMdFMTgjmctO6MXgjGQG90hiYPekkG+eCSSf7VnsKdgfVtXzPM8fAFDVP3hdswI4X1W3eJ5vAE5W1W1tvW92dra23Jhm1apVDBkypOO/RJAKt+9rjhyqSmlVHYVl1Wwtr6ao3I22KSjdTU5RBWu2VjS32UdGCEd1TWwu7IdkJDG4RzIZKXF2d+8DIrJQVVsdq+7LFDsfGCAi/YB8YCJwdYtrNgNnAS+KyBAgDij2YUzGmENU19BIcUUNReVuvHyRp7AvLKtuPra1vHqfzlkR6JYUy8DuSVw/ui+DeyQzOCOJ/umdgm4YZbjyWSJQ1XoRuR34CDc09HlVXSEijwALVHUm8DPgWRH5Ka7jeIr6qopijNmvxkZlS0kVqwrLWV9c2VzIb/UU8sW7avbppI2JiqBHchw9UuIY0TuVHilxzc+bHqcnxRLto4lQpmP4tNHNMydgdotjD3k9Xgmc6ssYjDH7qqqtZ3VRBasKyz0/rpO2srah+ZqU+OjmQn1Ij2S6e8bR90iOo3uye5yaEG3NOCHAel+MCWGqSmFZNSsLPAV+kSv0N3kNwUyKjWJIRjKXj8xkSEYyQzKSGdC9EwkxVjyEC/ubNiZEVNe5iVYrm+/yXaHvveZNny4JDMlI4pIRvRiSkcSQjGQyO8fbXX2Ys0RgzBGkbHcd+SW7yS/dTUGp+51fspu12ypYX1zZvHJlfHQkg3okceGxGQz1FPiDeiSRFGfLlZh9WSLoAIezDDXAnDlziImJ4ZRTTvF5rCZ4NTYq2ypqXOHuKeALWjyuqNl7pnlMVAS9UuPp1zWRc4f28DTtJNE3LZHICLvLN+1jiaADHGgZ6gOZM2cOnTp1skQQBnbV1LM0r5QtO6vIL6323N1XUeBZyriuYe9hOSnx0fRKjad3lwRG90+jV2o8PVPj6dU5nl6p8aQlxhBhBb45TJYIfGThwoXcc8897Nq1i65du/Liiy+SkZHB448/ztNPP01UVBRDhw7l0Ucf5emnnyYyMpKXX36ZJ554gtNOOy3Q4ZsOUli2m/mbSli4aScLcktYVVhO074jItAjOY6eqfGM6J3KuOMy6JkaT6anoO+ZGk8nm01r/CD0/pX9634oWtax79njWLjg0QNf56Gq3HHHHbz33nukp6fz+uuv88tf/pLnn3+eRx99lI0bNxIbG0tpaSmpqan85Cc/OehahAk+DY1KTlEFC3J3smBTCQtzS8gvdUsnxEdHcnyfVG4fezQn9O1M//RO9EiJs/H1JiiEXiIIAjU1NSxfvpxzzjkHgIaGBjIyMgA47rjjuOaaa7jkkku45JJLAhilOVxVtfUs3lzK/E0lLMjdyeLNpc1t+N2TY8nu24Ubx/TjxKwuDMlI8tnuUsYcrtBLBAdx5+4rqsqwYcOYO3fuPuc++OADvvrqK2bNmsXvf/97li3r4NqL8Zmt5dUs2FTC/E07WZhbwsrCchoaFREY1D2J8SN6kp3Vmey+XWxIpjmihF4iCAKxsbEUFxczd+5cRo8eTV1dHWvWrGHIkCFs2bKFsWPHMmbMGKZPn86uXbtISkqivLxjd1Uzh6e2vpFVheUsyStlUW4JC3JLmlfIjIuOYETvVG79QX9GZnXmhD6dSYm3YZnmyGWJwAciIiKYMWMGd955J2VlZdTX13P33XczcOBArr32WsrKylBV7rzzTlJTU7n44ou5/PLLee+996yzOABUldwdVSzJK+X7zaUs3lLKyoJyahvc4mnpSbFk9+3MlFOyyM7qwrCeyda2b0KKz5ah9hVbhjr8vm9H21lZy5K8UhZ7Cv0leaWUVrnZt/HRkRzbK4URfVIZnpnKiD6p9Ay3ZZFVQRshwlYGDSWBWobamICrrmtgZWH5XoV+7o4qwA3fHNgtifOG9mB471RG9E5lYPdO4d2p29gIM++Ale/CcVdC9g1u1JwJaZYITMiob2hk4/ZKluaVNRf6qwrLmydp9Uh2SyVPPLEPI3qncmxmio3Tb+nTh2Dxy9B3DCx+FRY8D5knuoQw7FKIjg90hMYHQuZ/gaqGRfX9SGvK85WyqjrPSpp7FlfL2VpBrWdTlMSYSI7LTOXGMUcxwnO33yMlLsBRB7n/PA7fPAGjboYL/gjVpbBkuksG794KHz4AI66GkT+C9IGBjtZ0oJDoI9i4cSNJSUmkpaWFdDJQVXbs2EFFRQX9+vULdDh+0dio5O6s8irwXaHfNFELIC0xpnmNnSEZyQzrmcLR3TrZWjsHY/Fr8O5P3F3/Zc9DhFfzmCrk/sclhJUzobEOsk6D7B/B4Ishav9raXWY8kIXx6Z/Q/5CGDIeTr/XtfEFm90lsP4LGHg+xCQEOhpg/30EIZEI6urqyMvLo7q6OkBR+U9cXByZmZlER4fecMVdNfXkFJWzsnDPhik5RRVUeTZLadrjtmnN/CEZSQzNSCY9KTakbwB8bs1H8NokyBoD17wJUbFtX7ur2DUdLXgBSnMhMR2OvxZGToHOWR0bV0URbPraFfybvoYd69zx2GTo0g8Kl8DwSXDx4/5LRu2xcwO8ciXsWAsJXeGU2+HEH0NsUkDDCvlEYI5MxRU1vL0oj+83l7KqqLy5ExcgKc5tljLU89O0WYrtcdvBtnwHU8dD+iCY8n77C6vGRtjwuUsIObNdreHos1xfwoDzIPIQWp2bC37Pz4617nhsMvQ9xSWqrDHQ4ziQCPjyjzDnv+GosXDlNIhLPvjP7Gib58H0SW7U1dm/gZXvwfrPIC4VTr4VTroF4jsHJDRLBCZoqCqLNpcybe4mZi8rpK5ByUpL8LrLd3f6vVJtZq7PbVsNz58HCV3gho+hU/qhvU9ZPnz/EiycChUFkNQTRk6GE66H5J5tv+5gCv62hrJ+/zLMvBO6DXW1meSMQ/sOHWH5W/DOrZDSC66ZAWn93fH8hfDVn1zCjEmCUTfB6Nsgsatfw7NEYAJud20DM5fkM21uLisKykmKjeKykZlcN7ov/dM7BTq88FOWB8+dC431cOPHHdOs01APaz9yfQnrPnN37YMucH0JR50JldtaL/hjkvYt+A+mRrHuU3hjsrvrvnYGdPPzHBtV+Pef4fPfQp/RMPFVl1xbKlrmrlvxLkTFudrTKXf4LXlZIjABs3lHFS/Py+X1+Vso213HoO5JXDe6L5ce34tEG7oZGFU74fnzoaIQfjTbN/MEdm6ERVNh0UtQtR3iUqC6zJ073IK/NQWL4dUrob7aFcRZYw77K7RLfS28/1PXb3LsFTDhyf33sQAUr4Gv/wJL33A1neOvgzF3Q2ofn4ZqicD4VWOj8uXaYl6am8sXOduIEOH8YT24bnRfTurXxZp8Aqm2EqZNgMKlcN07kHWqbz+vvhZWz4K1n7g79awx0GP44Rf8rSnJhVcuh5JNcOnTcMxlHf8Z3naXwhvXwcav4Ae/gDMeOLgRTDs3wNd/c/M1UDhuIpx2z54mpQ5micD4RVlVHW8u3MJL3+aSu6OK9KRYJo3qw9Wj+vhvDH/uN7BtFQy5GDp1889nHika6tzooPWfwZUvwZCLAh1Rx6vaCdOvhs1z4dzfwejbfTO8tGSTGxm0cwOMfwJGTDr09yrLc3M4Fk2FhlqXwE77WYc3cVkiMD61oqCMl+bm8u7ifKrrGjkxqzPXjc7i/GE9iIny43INNRXw2AjXFCER0O8HcOzlMPgiiE/1XxzBqLHRTQpbOh0ufswN9wxVddXwzs1uxM5Jt8J5v+/YdZPyFsBrE12hfdUr0K+DFoms2Apz/w/mPwd1le5m5rSfQ88RHfL2lghMh6utb+Rfywt5aW4uC3JLiIuO4NLje3HdyVkM7RmgYXxf/AG+fBQue87VCpbPcHdukTEw4Fw45ocw8IKgmeDjVx//ys0aHvsr+MG9gY7G9xob4eNfwrd/dxPPfvhMxyyPsfI9ePtmSOoBV7/pmxnWVTvh26dg3j+gpswNxz3959B71GG9rSUC02GKyqp5dV4ur363he27ashKS+Dak/tyxcjepCQEcJLbrmJ4fIQby37lNHdMFfIXuWF9K952naPRiTD4Qjjmcuh/ZnBNRPKV/zwOn/x6z9IR4dRHM/dJ+OiX0PskmPRa66N52kMVvnkcPnkIMke59/L18M/dpTD/WZj7d9i909Vwz3oIMlstyw/IEoE5LKrKvI07mTZ3Ex+t2EqjKmcO6sZ1o/ty+oB0IoJhKYfZ98H8f8Jt30HXo/c939jg+g+Wz3B3dbtL3HDDoeNdUsga49tllxsboTwPdqyH8gLoOxq6HOW7z2uy19IRz4Xn0tIr3oG3b3Gjcq6dcfBDZRvqYPbPYeGL7s/xkqf8u/hezS5Y+IJL6Bf+0cVwCCwRmENSWVPPu4vzmfZNLjlbK0iJj+aqE3tz3cl96d0liJpXSjbBE9lw/DWu/ftA6mthwxewbAas/sC1x3bq4f6DHXs59Bp5aHfNjY1uQtWO9bBzvetI3LHB83gjNNTsfX3fU2HENTB0AsT6YC7FwSwdEepyv3F/FpExcM0b0PP49r2uugzenALrP4cx98CZv957HSZ/qtvt4j/EZG6JwByUjdsreWluLm8u3EJFdT3DeiYzeXQWFw/vSXxMEN5Rvu3pGLxz8cFPzqmtcpOgls2AtR+7DsDOWW7kxjGXQfdhe1+v6pqYmgr7HZ4Cv+mn3mu9q8hYd9ff5ShIOwq69HdDAxPT3SzT719x7xGdCMMucUmh7ykd03RzqEtHhLLiHHj5cqjaAVdOhQHn7P/60i1ubsL2NXDR3+CE6/wSpq9YIjAH1NCozMnZxtS5uXy1ppjoSOHCYzO4fnQWJ/RJDd6x/0XL4ekxcOpdcM5vDu+9qstg1fuu+WjDl6ANkD4E+p3uCv+mwr5uz5pIRMa4xNFUyHc5yvO7PyT32v/doypsmQeLX4Hl70BtBXTu5xLCiEmQknlo36Ojlo4IRRVF8MoVsHUFXPw3twxGa/IXuZFBddVw1TQ46gx/RukTlghMm0qranljgRv7v2Xnbronx3LNSX2ZOKo33ZKOgPX7X7nCFaZ3LenYxbx2Fbtdupa/5QqF1N6tF/YpmR3T7l5bCatmubVzNv0bEFf4HH8tDB7X/jZpXywdEWpqKuCN611zzw/uhzPu37sWtup9eOvHLoFe/SZ0Gxy4WDtQwBKBiJwPPAZEAv9U1UdbnP8rMNbzNAHopqqp+3tPSwQdY3n+nrH/NfWNjOrXhcmjszh3WPcjZ2P2Tf+BFy90qzyOuTvQ0XSckk2uk3fxq1C2GWJT4NjLXE1hf/0X/lg6IlQ01MGsu1xtbMS1rnYQEeWGm370S+h1AkyaHlKTEgOSCEQkElgDnAPkAfOBSaq6so3r7wCOV9Ub9ve+lggOXdPY/2lzc1mYW0J8dCSXntCL60f3ZXCPIFjC92CownPnuJUv71wUmlsoNja62sHiV9yGMPW7IX2w2yXsuImQ1H3PtXstHfG2/9baOZKpwpw/wJf/A/3PcrWnBc+5iVyXPhNy800CtXn9KGCdqm7wBDEdmAC0mgiAScB/+TCesLW1vJpX5m3m1Xmbm8f+//qioVw+MpOU+CN0g5uc2ZA3321KEopJAFz/wlE/cD8X/q8bBvn9K24s+6e/cZ2dI65xcyfemOyWO75ymiWB9hKBsQ+6vpz3f+r6hE6509UwAzUyKEB8mQh6AVu8nucBJ7V2oYj0BfoBn7dx/mbgZoA+fXy7Ql8oWVVYzv99sY6PlhfRoMrYQd24PpjG/h+qxgb47BFIG+AKwnAQl+KWhRg5xa1eufgVt5/wmg/d6KSGGjd0dsjFgY70yDNysuvzqdrhhvKGoWBZB3giMENVG1o7qarPAM+AaxryZ2BHotr6Rv7v87X8fc56EmIi+dGpWVx7cl/6piUGOrSOseQ1KF7t7n59sYplsEsf6EZInflrNx9i6evQKzu01w/ytTCvRfnyf1E+0NvreabnWGsmArf5MJawsTSvlHvfXErO1gp+eHwvHrp4KKkJIbSMQl21W1Oo10i3hkw4i4xyzUMHGg9vzAH4MhHMBwaISD9cApgIXN3yIhEZDHQG5vowlpBXXdfAY5+t5ZmvNtC1UwzPT8nmzMHdD/zCI838Z91SDZc+FV5r5hjjQz5LBKpaLyK3Ax/hho8+r6orROQRYIGqzvRcOhGYrkfahIYgsmhzCffNWMq6bbu4Krs3D44bcuR2Au9PdZnb6q//WW6SlzGmQ/i0gVVVZwOzWxx7qMXzh30ZQyirrmvgzx/n8NzXG8lIiWfaDaM4fWAIzyL9z2NusbizbXCZMR0pDHvaQsP8TTu5b8ZSNm6v5JqT+nD/BYNJivNxLaC+1g3Z7DPa/8PrKorcGu3HXAYZw/372caEOEsER5iq2nr++GEOU+duIrNzPK/++CROOdrH66KD2+jl7ZugaJlbn+Wix/ybDL78o1sQbuwv/feZxoQJSwRHkG/Wb+cXby1ly87dTDkli3vPG0RirI//ChsbYd7T8OnDbgXL4VfDomluiv6EJ/2zvv2O9W4/15FTfLaxtzHhzBLBEWBXTT1/mL2KV+ZtJistgTduGc2ofoe409LBKMt3+9xu/BIGnu826e7UzS269sXv3B36pf+ASB83SX3+O7fK5+n3+fZzjAlTlgiC3Fdrinng7WUUlO3mptP6cc85g/yzJ8CyGfDBPdBQ72asnjB5z3DNH9zrtnj85CGXDC573ndbPhZ877aZPP3evdfWMcZ0GEsEQapsdx2//2AlbyzIo396Im/degon9OnAZZbbsrvUbcu37E3IPNHd8bfWHHPqXe4u/cP73ZK+V071zQ5Yn/4G4rvAKXd0/HsbYwBLBEHp89VbefDt5WyrqObWM/pz11kDiIv2Qy1gw5euKaiiyHXKjrln/0s4nHyrSwYf3APTr4arXu7YBeA2zHFLKJz7e7fWjjHGJywRBJHSqloembWSt7/PZ1D3JJ65fiTHZab6/oPrquHz38Lc/4O0o+HHn7glHNrjxBtdMph5B7x6FUx6DWI6YE0jVVcbSM6EE398+O9njGmTJYIgsWl7Jdf8cx5by6u588yjue3Mo4mN8kMtoGiZ2/N320pX4J7z24Nfh/2E61wyePcnbsewq18//D1yV74HBYtgwt8h+gjYKc2YI5glgiCwblsFVz87j7qGRt669RSG9071/Yc2NrgawOe/c1s8XjPj8BYvG36Va0Z66yZ46Ydw7YxDb85pqHc1lPTBMHziocdkjGkXSwQBtrKgnOuem4eI8PotoxnY/TDvpNujdDO8cyvkfu3Wr7/oMUhMO/z3PeYyiIiGGTfAtEvcTlmHso/w9y/BjnUw8VX/zFMwJsyF1zY8QWZpXimTnv2WmKgI3rjlZN8nAVW3mclTp0LhErjkKbjypY5JAk2GjnedxluXw9TxULnj4F5fWwVzHoXeJ8GgCzsuLmNMmywRBMjC3J1c8+w8kuOjeOOW0RyV3sm3H1i1E96cAu/cAt2Hwa1fu71vfbGU86DzXafx9jUw9WLYVdz+1857GnYVwdkP2zLTxviJJYIAmLt+B9c99x3pSbG8cctoenfx8SbZ6z6Dv4+G1R/AWf8FUz5wG3X70tFnu07jnRvgxXFuSOqB7C6B//wNBpwHfU/xbXzGmGaWCPzsyzXFTHnhOzI7xzP9lpPJSPHhxuu1lTD7Pnj5hxCfCjd9Bqfd479296POgGvfgvJ8eOFCt2TF/nz9V6guh7Me2v91xpgOZYnAjz5eUcRNUxfQP70T028eTbckHwyLrK2EFe+4ZqD/PRq++wec/P/g5jmBWb4561S47h2oLIYXLoCS3NavK8uHef+A466EHsf4N0ZjwpyNGvKT95cWcPf0xRzTK4WpPxpFSkIHLtRWswvWfgwr34W1n0BdFSSmu6GXw6+G3id23Gcdit6j4Pp34aVLXTPR5Jlu4TpvXz7qhrSOfTAgIRoTziwR+MFbC/O4d8YSsvt24bkp2R2zgUzNLljzoafw/xTqd0NiNxg+CYZdAn1PDa6hl71GwuRZbljpC55k0HWAO1e8Br5/GUbd7Pu+C2PMPiwR+Nir8zbz4DvLGHN0V565fiQJMYfxR15TAWs+ck0/6z6F+mro1B2Ov9YV/n1GB1fh31LGcJjyPkyb4PoMJs+CboPh80cgOgFO+3mgIzQmLFki8KHnv97II++v5MzB3fj7NScc2sJx1eXuzn/Fu67wb6iBpAy3LPSwS9x4+2Au/FvqPsyNWpo6Hl680I1iWjULzngAOoXwfsvGBDFLBIdjw5furjw6wa3PE53Y/Psf3xbxh082cv6wDB6fdDwxUQfRL19dBjn/cuvtrPvMU/j3hOwbYOgET+F/BPfzpw+CH812cwxm3QkJXWH0bYGOypiwJaoa6BgOSnZ2ti5YsCDQYcDWlfDU6P1e0kgEEpOIxCR4kkVii6SRuHcCiYpzm8Ov/9xt+JLcyxX8Qy9xewMcyYV/a0o2wYwb4aRb3GghY4zPiMhCVc1u7ZzVCA7V1uXu9+UvQGJXqK1CayuZvWgd89fkcVJmHOcNSEbqq9yQzroqt3xCXaX7vat4z+M6zzUopPR2naZDL3EdrKFW+HvrnOXmNhhjAsoSwaEqzgGJhMEXQVQMjY3Kb2atYOqqOK47eRznjR9GRMRBLJGg6pqZouJsaQVjjF9ZIjhUxavdWPioGBoalV++s4zp87dw02n9ePDCIcjBFuYiHbu7lzHGtJMlgkNVnAPpg6hvaOTnby7h3cUF3Hnm0fz0nIEHnwSMMSaAQrgB2ofqa2HnBhrSBnHHa9/z7uIC7j1vEPecO8iSgDHmiHPARCAiF4uIJQxvO9eDNvDcmhj+tbyIX180lNvGHh3oqIwx5pC0p4C/ClgrIn8UkcG+DuiIULwagPfykvjVuCHcOKZfgAMyxphDd8BEoKrXAscD64EXRWSuiNwsIn7YUzFIFefQiLAzrg+TT8kKdDTGGHNY2tXko6rlwAxgOpABXAosEpE7fBhb0Krfupo87cbY47KIjrRWM2PMka09fQTjReQdYA4QDYxS1QuA4cDPDvDa80UkR0TWicj9bVxzpYisFJEVIvLqwX8F/6vMX8Haxp6MH94z0KEYY8xha8/w0cuAv6rqV94HVbVKRG5s60UiEgk8CZwD5AHzRWSmqq70umYA8ABwqqqWiEi3Q/kSftVQT0L5Rgqjx3F1VpdAR2OMMYetPe0aDwPfNT0RkXgRyQJQ1f2tDzAKWKeqG1S1FtesNKHFNTcBT6pqief9trU/9MAoL1xLNHWk9D3m4GYOG2NMkGpPIngTaPR63uA5diC9gC1ez/M8x7wNBAaKyH9E5FsROb+1N/J0Ti8QkQXFxcXt+GjfWfL9PACGHDsqoHEYY0xHaU8iiPLc0QPgeRzTQZ8fBQwAzgAmAc+KSGrLi1T1GVXNVtXs9PTArlmfv2YxAP2HjghoHMYY01HakwiKRWR80xMRmQBsb8fr8oHeXs8zPce85QEzVbVOVTcCa3CJIShtLa8mtnQtFTHdkLiUQIdjjDEdoj2J4CfAgyKyWUS2AL8AbmnH6+YDA0Skn4jEABOBmS2ueRdXG0BEuuKaija0L3T/e39pIf0ln8juQwIdijHGdJgDjhpS1fXAySLSyfN8V3veWFXrReR24CMgEnheVVeIyCPAAlWd6Tl3roisxPU93KuqOw7xu/jcrMV5XBNRQFyvCwIdijHGdJh2rT4qIuOAYUBc06JqqvrIgV6nqrOB2S2OPeT1WIF7PD9BLXdHJdvz1xMXWwNdBwY6HGOM6TDtmVD2NG69oTsAAa4A+vo4rqAza0kBR0uee5JuSy4ZY0JHe/oITlHV64ESVf0NMBrXlh82VJX3FhcwNq3EHUgfFNiAjDGmA7UnEVR7fleJSE+gDrfeUNhYXVTB2m27ODV5OySmQ4LNKDbGhI729BHM8ozt/19gEaDAs74MKtjMXFJAZITQt3GLNQsZY0LOfhOBZ0Oaz1S1FHhLRN4H4lS1zB/BBQNVZdaSAk7tn0b0trVw3JWBDskYYzrUfpuGVLURt3Bc0/OacEoCAIs2l5JXspurBkdDTbnVCIwxIac9fQSfichlEqab8c5aUkBsVARj03a6AzZ01BgTYtqTCG7BLTJXIyLlIlIhIuU+jiso1Dc08v7SQs4c3I2E0nXuoNUIjDEhpj0zi8N2S8pvN+xk+64atwFNbg7EpUKn4N8ywRhjDsYBE4GInN7a8ZYb1YSi9xbn0yk2irGDu8GCHDd/IDxbyIwxIaw9w0fv9Xoch9twZiFwpk8iChI19Q18uKKIc4d1Jy46EopXw+BxgQ7LGGM6XHuahi72fi4ivYG/+SqgYDEnp5iK6nrXLFS5Hap2WP+AMSYktaezuKU8IOTXYZ65pIC0xBhOPborFOe4g7a0hDEmBLWnj+AJ3GxicIljBG6GcciqrKnns1VbuWJkb6IjI1yzEEBXSwTGmNDTnj6CBV6P64HXVPU/PoonKHyycivVdY2MH9HTHSjOgZhOkJIZ2MCMMcYH2pMIZgDVqtoAICKRIpKgqlW+DS1w3lucT8+UOEb26ewObM9xE8lsxJAxJgS1a2YxEO/1PB741DfhBF5JZS3/Xrudi4f3JCLCU/AX51hHsTEmZLUnEcR5b0/peZzgu5ACa/byQuoblYuHe5qFdpdCRSGk29ISxpjQ1J5EUCkiJzQ9EZGRwG7fhRRYMxcX0D89kWE9k92B7Wvcb6sRGGNCVHv6CO4G3hSRAtxWlT1wW1eGnKKyar7btJO7zxpI8xp7NnTUGBPi2jOhbL6IDAaaSsIcVa3zbViB8f7SAlTZM1oI3NDRyFhIDbttmo0xYaI9m9ffBiSq6nJVXQ50EpH/5/vQ/G/mkgKO7ZVCv66Jew4We0YMRUQGLjBjjPGh9vQR3OTZoQwAVS0BbvJZRAGycXslS/PK3JIS3opzrFnIGBPS2pMIIr03pRGRSCDGdyEFxszFBYjARcMz9hysrYSyzdZRbIwJae3pLP4QeF1E/uF5fgvwL9+F5H+qyswl+YzK6kJGiteUieYRQzZ01BgTutqTCH4B3Az8xPN8KW7kUMhYWVjO+uJKbhjTb+8TzSOGrEZgjAldB2wa8mxgPw/YhNuL4ExglW/D8q+ZSwqIihAuPCZj7xPFORARBV2OCkxgxhjjB23WCERkIDDJ87MdeB1AVcf6JzT/aGxU3l9SyGkDutI5sUXXR3EOpB0NkdGBCc4YY/xgfzWC1bi7/4tUdYyqPgE0+Ccs/1m0uYT80t17zx1oUrzaDR01xpgQtr9E8EOgEPhCRJ4VkbNwM4tDynuLC4iNiuCcoS26PeqqoWSj9Q8YY0Jem4lAVd9V1YnAYOAL3FIT3UTkKRE510/x+VR9QyOzlxVy9tDudIpt0Uq2cz1oo80hMMaEvPZ0Fleq6quevYszge9xI4kOSETOF5EcEVknIve3cn6KiBSLyGLPz48P+hschv+s38GOytp9J5HBnl3JLBEYY0Jce4aPNvPMKn7G87NfnolnTwLn4PY5ni8iM1V1ZYtLX1fV2w8mjo4yc3EBSXFRnDEofd+TxTkgEa6z2BhjQtihbF7fXqOAdaq6QVVrgenABB9+3kGprmvg4xVFnD+sB7FRrawjVJwDnbMgOn7fc8YYE0J8mQh6AVu8nud5jrV0mYgsFZEZItK7tTcSkZtFZIGILCguLu6Q4ObkbKOipr710UJgu5IZY8KGLxNBe8wCslT1OOATYGprF6nqM6qararZ6emtNOMcgplLCujaKYbRR6Xte7KhDnass6Gjxpiw4MtEkA943+Fneo41U9UdqlrjefpPYKQP42lWUV3Hp6u2Me7YDKIiW/kj2LkRGuusRmCMCQu+TATzgQEi0k9EYoCJwEzvC0TEe02H8fhp6YqPV2yltr6R8SNaa6kCttuuZMaY8HFQo4YOhqrWi8jtwEdAJPC8qq4QkUeABao6E7hTRMYD9cBOYIqv4vE2c0kBmZ3jOaFPausXNA0dtaYhY0wY8FkiAFDV2cDsFsce8nr8APCAL2NoaceuGr5et52bTz8Kr20W9lacAym9IbaTP0MzxpiACHRnsd/NXl5EQ6O2PomsSfFqaxYyxoSNsEsEsxYXMKBbJwb3SGr9gsYG2L7WOoqNMWEjrBJBfuluvtu0k/HDe7bdLFS6GeqrrX/AGBM2wioRvL+kAKDtSWRgu5IZY8JOWCWCmUsKGN47lb5piW1f1Dx01GoExpjwEDaJYH3xLlYUlO+/kxhcjaBTD4jv7J/AjDEmwMImEcxaUoAIXHRcxv4vLF5ttQFjTFjx6TyCYHLDmH4Mz0yle3Jc2xepQvEaGDHJf4EZY0yAhU2NIDkumrGDu+3/ovICqK2wOQTGmLASNomgXZqXlrBEYIwJH5YIvNnQUWNMGLJE4G17DsR3gcSugY7EGGP8xhKBt6ZdydqadWyMMSHIEkETVdi2yoaOGmPCjiWCJpXFUF1q/QPGmLBjiaBJse1KZowJT5YImjQNHbUagTEmzFgiaFKcAzFJkHSAJSiMMSbEWCJo0rQrmY0YMsaEGUsETbavsWYhY0xYskQAULUTdm21oaPGmLBkiQBcbQCsRmCMCUuWCMCGjhpjwpolAnCJICoeUvoEOhJjjPE7SwTgRgx1HQAR9sdhjAk/VvLBnsXmjDEmDFkiqKmA8jzrHzDGhC1LBDZiyBgT5iwR2IghY0yYs0RQnAMR0dC5X6AjMcaYgLBEUJzjRgxFRgU6EmOMCQifJgIROV9EckRknYjcv5/rLhMRFZFsX8bTquLV0NWWljDGhC+fJQIRiQSeBC4AhgKTRGRoK9clAXcB83wVS5vqdkPJJusoNsaENV/WCEYB61R1g6rWAtOBCa1c91vgf4BqH8bSuh3rALWOYmNMWPNlIugFbPF6nuc51kxETgB6q+oH+3sjEblZRBaIyILi4uKOi7B5xJDVCIwx4StgncUiEgH8BfjZga5V1WdUNVtVs9PT0zsuiOLVIBGQ1r/j3tMYY44wvkwE+UBvr+eZnmNNkoBjgDkisgk4GZjp1w7j4tXQ5SiIivXbRxpjTLDxZSKYDwwQkX4iEgNMBGY2nVTVMlXtqqpZqpoFfAuMV9UFPoxpb8W2K5kxxvgsEahqPXA78BGwCnhDVVeIyCMiMt5Xn9tu9bWwc70NHTXGhD2fzqJS1dnA7BbHHmrj2jN8Gcs+dm6AxnqrERhjwl74zizebmsMGWMMhHMiKM4BxJqGjDFhL4wTwWpI7Q0xCYGOxBhjAiqME4HtSmaMMRCuiaCxAbavtf4BY4whXBNBySZoqLEagTHGEK6JoGmNoa5WIzDGmPBMBM1DR23EkDHGhGciKM6BpJ4QlxLoSIwxJuDCNBGsttqAMcZ4hF8iaGy0xeaMMcZL+CWC8nyoq7Sho8YY4xF+icB2JTPGmL2EYSJY7X7b0FFjjAHCNREkdIXEtEBHYowxQSH8EsF26yg2xhhv4ZUIVD1DR61ZyBhjmoRXIti1FarLLBEYY4yX8EoExbYrmTHGtBSmicD6CIwxpkmYJYLVEJsCnboHOhJjjAkaYZYIclyzkEigIzHGmKARXolge471DxhjTAvhkwgqd0BlsfUPGGNMC+GTCLbbiCFjjGlN+CQCGzpqjDGtCp9E0KkbDBoHyZmBjsQYY4JKVKAD8JvB49yPMcaYvYRPjcAYY0yrLBEYY0yYs0RgjDFhzqeJQETOF5EcEVknIve3cv4nIrJMRBaLyNciMtSX8RhjjNmXzxKBiEQCTwIXAEOBSa0U9K+q6rGqOgL4I/AXX8VjjDGmdb6sEYwC1qnqBlWtBaYDE7wvUNVyr6eJgPowHmOMMa3w5fDRXsAWr+d5wEktLxKR24B7gBjgzNbeSERuBm4G6NOnT4cHaowx4SzgncWq+qSq9gd+AfyqjWueUdVsVc1OT0/3b4DGGBPifFkjyAd6ez3P9Bxry3TgqQO96cKFC7eLSO4hxtQV2H6Ir/WXYI8x2OMDi7EjBHt8EPwxBlt8fds64ctEMB8YICL9cAlgInC19wUiMkBV13qejgPWcgCqeshVAhFZoKrZh/p6fwj2GIM9PrAYO0KwxwfBH2Owx+fNZ4lAVetF5HbgIyASeF5VV4jII8ACVZ0J3C4iZwN1QAkw2VfxGGOMaZ1P1xpS1dnA7BbHHvJ6fJcvP98YY8yBBbyz2M+eCXQA7RDsMQZ7fGAxdoRgjw+CP8Zgj6+ZqNrQfWOMCWfhViMwxhjTgiUCY4wJc2GTCA60AF4giUhvEflCRFaKyAoRCdpOdBGJFJHvReT9QMfSGhFJFZEZIrJaRFaJyOhAx+RNRH7q+TteLiKviUhcEMT0vIhsE5HlXse6iMgnIrLW87tzEMb4v56/56Ui8o6IpAZTfF7nfiYiKiJdAxFbe4RFImjnAniBVA/8TFWHAicDtwVZfN7uAlYFOoj9eAz4UFUHA8MJolhFpBdwJ5CtqsfghlVPDGxUALwInN/i2P3AZ6o6APjM8zyQXmTfGD8BjlHV44A1wAP+DsrLi+wbHyLSGzgX2OzvgA5GWCQC2rEAXiCpaqGqLvI8rsAVXr0CG9W+RCQTN/Hvn4GOpTUikgKcDjwHoKq1qloa0KD2FQXEi0gUkAAUBDgeVPUrYGeLwxOAqZ7HU4FL/BlTS63FqKofq2q95+m3uNULAqKNP0OAvwL3EeQLaoZLImhtAbygK2gBRCQLOB6YF+BQWvM33D/qxgDH0ZZ+QDHwgqf56p8ikhjooJqoaj7wJ9zdYSFQpqofBzaqNnVX1ULP4yKgeyCDaYcbgH8FOghvIjIByFfVJYGO5UDCJREcEUSkE/AWcHeLJboDTkQuArap6sJAx7IfUcAJwFOqejxQSeCbNJp52tkn4BJWTyBRRK4NbFQHpm6MedDe0YrIL3HNq68EOpYmIpIAPAg8dKBrg0G4JIKDXQDP70QkGpcEXlHVtwMdTytOBcaLyCZc09qZIvJyYEPaRx6Qp6pNtakZuMQQLM4GNqpqsarWAW8DpwQ4prZsFZEMAM/vbQGOp1UiMgW4CLhGg2tSVH9cwl/i+T+TCSwSkR4BjaoN4ZIImhfAE5EYXAfdzADH1ExEBNeuvUpVg3KXNlV9QFUzVTUL9+f3uaoG1d2sqhYBW0RkkOfQWcDKAIbU0mbgZBFJ8Pydn0UQdWa3MJM9a39NBt4LYCytEpHzcU2V41W1KtDxeFPVZaraTVWzPP9n8oATPP9Gg05YJAJPh1LTAnirgDdUdUVgo9rLqcB1uLvsxZ6fCwMd1BHqDuAVEVkKjAD+O7Dh7OGpqcwAFgHLcP//Ar4MgYi8BswFBolInojcCDwKnCMia3E1mUeDMMb/A5KATzz/Z54OsviOGLbEhDHGhLmwqBEYY4xpmyUCY4wJc5YIjDEmzFkiMMaYMGeJwBhjwpwlAmNaEJEGr2G8iztytVoRyWpthUpjAsmnexYbc4TaraojAh2EMf5iNQJj2klENonIH0VkmYh8JyJHe45nicjnnnXxPxORPp7j3T3r5C/x/DQtJxEpIs969iX4WETiA/aljMESgTGtiW/RNHSV17kyVT0WN6v1b55jTwBTPevivwI87jn+OPClqg7HrXnUNJt9APCkqg4DSoHLfPptjDkAm1lsTAsisktVO7VyfBNwpqpu8CwSWKSqaSKyHchQ1TrP8UJV7SoixUCmqtZ4vUcW8IlnwxdE5BdAtKr+zg9fzZhWWY3AmIOjbTw+GDVejxuwvjoTYJYIjDk4V3n9nut5/A17tpy8Bvi35/FnwK3QvNdzir+CNOZg2J2IMfuKF5HFXs8/VNWmIaSdPSub1gCTPMfuwO2Kdi9uh7QfeY7fBTzjWYmyAZcUCjEmyFgfgTHt5OkjyFbV7YGOxZiOZE1DxhgT5qxGYIwxYc5qBMYYE+YsERhjTJizRGCMMWHOEoExxoQ5SwTGGBPm/j/l05ERerTdRQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5n0lEQVR4nO3deXyU5bn/8c+VfSUhIYRAAmFVQDaNImhV6nFf21ortYpWa7FVu9vanlaPbU9tT6vV2tba1rrW5afiUleqIlhADcoiO7ImhBASspP9+v1xPwlDmGwwk0ky1/v1yiszz/PMzBWXfHMvz32LqmKMMca0FxHqAowxxvRNFhDGGGP8soAwxhjjlwWEMcYYvywgjDHG+GUBYYwxxi8LCGOOgojkioiKSFQ3rr1GRN472vcxprdYQJiwISLbRaRBRIa0O/6x98s5N0SlGdMnWUCYcLMNmNv6RESmAAmhK8eYvssCwoSbx4CrfZ7PAx71vUBEUkTkUREpEZEdIvLfIhLhnYsUkd+KyD4R2Qpc4Oe1fxeRIhEpFJFfiEhkT4sUkeEi8pKIlInIFhH5ms+5k0QkX0QqRaRYRO72jseJyOMiUioi5SLyoYhk9vSzjWllAWHCzXJgkIhM9H5xXwE83u6aPwApwBjgdFygXOud+xpwITADyAMua/fah4EmYJx3zdnA9UdQ51NAATDc+4z/FZHPeufuBe5V1UHAWOAZ7/g8r+4cIB2YDxw4gs82BrCAMOGptRVxFrAeKGw94RMat6lqlapuB34HXOVdcjnwe1XdpaplwK98XpsJnA98W1VrVHUvcI/3ft0mIjnAKcAPVbVOVVcCf+Ngy6cRGCciQ1S1WlWX+xxPB8aparOqrlDVyp58tjG+LCBMOHoM+DJwDe26l4AhQDSww+fYDmCE93g4sKvduVajvNcWeV085cBfgKE9rG84UKaqVR3UcB0wAdjgdSNd6PNzvQE8JSK7ReQ3IhLdw882po0FhAk7qroDN1h9PvB8u9P7cH+Jj/I5NpKDrYwiXBeO77lWu4B6YIiqpnpfg1R1cg9L3A2kiUiyvxpUdbOqzsUFz6+BZ0UkUVUbVfV/VHUSMBvXFXY1xhwhCwgTrq4DPquqNb4HVbUZ16f/SxFJFpFRwHc5OE7xDHCLiGSLyGDgRz6vLQLeBH4nIoNEJEJExorI6T0pTFV3AUuBX3kDz1O9eh8HEJGviEiGqrYA5d7LWkRkjohM8brJKnFB19KTzzbGlwWECUuq+qmq5ndw+magBtgKvAf8E3jIO/dXXDfOKuAjDm+BXA3EAOuA/cCzQNYRlDgXyMW1JhYAt6vqv71z5wJrRaQaN2B9haoeAIZ5n1eJG1t5F9ftZMwREdswyBhjjD/WgjDGGOOXBYQxxhi/LCCMMcb4FbSAEJEcEXlHRNaJyFoR+Zafa64UkdUiskZElorINJ9z273jK0Wko8FEY4wxQRLMpYWbgO+p6kfefO4VIrJQVdf5XLMNOF1V94vIecCDwEyf83NUdV93P3DIkCGam5sbiNqNMSYsrFixYp+qZvg7F7SA8OaEF3mPq0RkPe5O0HU+1yz1eclyIPtoPjM3N5f8fGtsGGNMd4nIjo7O9coYhLfO/gzg/U4uuw54zee5Am+KyAoRuaGT977BW9kyv6SkJCD1GmOMCW4XEwAikgQ8h1vAzO/CYSIyBxcQp/ocPlVVC0VkKLBQRDao6uL2r1XVB3FdU+Tl5dlNHcYYEyBBbUF4C4U9Bzyhqu3vOG29ZipupcpLVLW09biqtq47sxd3J+lJwazVGGPMoYLWghARAf4OrFfVuzu4ZiRuqYKrVHWTz/FEIMIbu0jEral/55HU0djYSEFBAXV1dUfy8n4lLi6O7OxsoqNtAU9jzNELZhfTKbg19NeIyErv2I/xVr9U1QeAn+HWr/+TyxOaVDUPyAQWeMeigH+q6utHUkRBQQHJycnk5ubivd+ApKqUlpZSUFDA6NGjQ12OMWYACOYspveATn8jq+r1+NltS1W3AtMOf0XP1dXVDfhwABAR0tPTsYF6Y0yghMWd1AM9HFqFy89pjOkdYREQnVKFqj1QZzszGmOMLwsIgOq9UFcR8LctLS1l+vTpTJ8+nWHDhjFixIi25w0NDZ2+Nj8/n1tuuSXgNRljTHcF/T6IPk8EomKhKfCznNLT01m5ciUAd9xxB0lJSXz/+99vO9/U1ERUlP9/BXl5eeTl5QW8JmOM6S5rQYALiObO/6IPlGuuuYb58+czc+ZMbr31Vj744ANmzZrFjBkzmD17Nhs3bgRg0aJFXHih24v+jjvu4Ktf/SpnnHEGY8aM4b777uuVWo0x4S2sWhD/8/Ja1u32M9bQ3OC+Yno+DjFp+CBuv6hne9IXFBSwdOlSIiMjqaysZMmSJURFRfHvf/+bH//4xzz33HOHvWbDhg288847VFVVccwxx3DjjTfa/Q7GmKAKq4DokHgNKW05+DiIvvjFLxIZGQlARUUF8+bNY/PmzYgIjY2Nfl9zwQUXEBsbS2xsLEOHDqW4uJjs7KNa29AYYzoVVgHR4V/6DbWwbyMMHg3xqUGvIzExse3xT3/6U+bMmcOCBQvYvn07Z5xxht/XxMbGtj2OjIykqakp2GUaY8KcjUGAG4MAaKrv9Y+uqKhgxIgRADz88MO9/vnGGNMRCwiAiEiIiILm3g+IW2+9ldtuu40ZM2ZYq8AY06eI6sBZITsvL0/bbxi0fv16Jk6c2PWL93lrBQ6ZEITKek+3f15jjAFEZIW3Bt5hrAXRKjI2JF1MxhjTV1lAtIqKhZYmaGkOdSXGGNMnWEC0iopz360VYYwxgAXEQa0zmUIwUG2MMX2RBUSryBj33VoQxhgDWEAcFBEJEdEWEMYY4wmrO6m7FOBVXUtLSznzzDMB2LNnD5GRkWRkZADwwQcfEBMT0+nrFy1aRExMDLNnzw5YTcYY010WEL6iYuFAecDerqvlvruyaNEikpKSLCCMMSFhXUy+ouJAm6E5eHc0r1ixgtNPP50TTjiBc845h6KiIgDuu+8+Jk2axNSpU7niiivYvn07DzzwAPfccw/Tp09nyZIlQavJGGP8CVoLQkRygEeBTECBB1X13nbXCHAvcD5QC1yjqh955+YB/+1d+gtVfeSoi3rtR7BnTcfnW5qg6QBEJ4BEdu89h02B8+7q1qWqys0338yLL75IRkYGTz/9ND/5yU946KGHuOuuu9i2bRuxsbGUl5eTmprK/Pnze9zqMMaYQAlmF1MT8D1V/UhEkoEVIrJQVdf5XHMeMN77mgn8GZgpImnA7UAeLlxWiMhLqro/iPW2W/a7mwHRA/X19XzyySecddZZADQ3N5OVlQXA1KlTufLKK7n00ku59NJLA/7ZxhjTU0ELCFUtAoq8x1Uish4YAfgGxCXAo+oWhFouIqkikgWcASxU1TIAEVkInAs8eVRFdfWXvrZA0SpIyoRBw4/qo/y+vSqTJ09m2bJlh5175ZVXWLx4MS+//DK//OUvWbOmk5aOMcb0gl4ZgxCRXGAG8H67UyOAXT7PC7xjHR0PLolw90MEaaprbGwsJSUlbQHR2NjI2rVraWlpYdeuXcyZM4df//rXVFRUUF1dTXJyMlVVVUGpxRhjuhL0gBCRJOA54Nuq2vM9Pbt+/xtEJF9E8ktKSo7+DaNig3Y3dUREBM8++yw//OEPmTZtGtOnT2fp0qU0Nzfzla98hSlTpjBjxgxuueUWUlNTueiii1iwYIENUhtjQiKo01xFJBoXDk+o6vN+LikEcnyeZ3vHCnHdTL7HF/n7DFV9EHgQ3HLfR110VBzUloIqiBz127W644472h4vXrz4sPPvvffeYccmTJjA6tWrA1aDMcb0RNBaEN4Mpb8D61X17g4uewm4WpyTgQpv7OIN4GwRGSwig4GzvWPBFxnrxiJabPMeY0x4C2YL4hTgKmCNiKz0jv0YGAmgqg8Ar+KmuG7BTXO91jtXJiI/Bz70Xndn64B10PluPxoZ3SsfaYwxfVEwZzG9B3TaR+PNXvpmB+ceAh4KUC1Id7uL2gKiDmKTAvHxvWYg7Q5ojAm9AX8ndVxcHKWlpd3/5RkZA0i/W/ZbVSktLSUuLi7UpRhjBogBvxZTdnY2BQUF9GiGU1UZRFRBYsAnXQVVXFwc2dnZoS7DGDNADPiAiI6OZvTo0T170ZN3wv7t8I3Db2gzxphwMeC7mI5I2hgo2wotLaGuxBhjQsYCwp/0cW6QurIw1JUYY0zIWED4kz7WfS/dEto6jDEmhCwg/Ekf576XfRraOowxJoQsIPxJznJ7QpRaQBhjwpcFhD8ikDbWAsIYE9YsIDqSPsa6mIwxYc0CoiNpY929EEHcn9oYY/oyC4iOpI9zK7qW7wh1JcYYExIWEB1pm+pq3UzGmPBkAdERm+pqjAlzFhAdSUiH2BS7Wc4YE7YsIDoi4rqZrIvJGBOmLCA6kz7WupiMMWHLAqIzaWOhfBc01oW6EmOM6XUWEJ1JHweoux/CGGPCjAVEZ9LHuO82UG2MCUMWEJ1J8+6FsHEIY0wYCtqWoyLyEHAhsFdVj/Nz/gfAlT51TAQyVLVMRLYDVUAz0KSqecGqs1PxqZAwxFoQxpiwFMwWxMPAuR2dVNX/U9XpqjoduA14V1XLfC6Z450PTTi0Sh8HpVtDWoIxxoRC0AJCVRcDZV1e6MwFngxWLUfFproaY8JUyMcgRCQB19J4zuewAm+KyAoRuaGL198gIvkikl9SUhL4AtPHQlUR1FcH/r2NMaYPC3lAABcB/2nXvXSqqh4PnAd8U0RO6+jFqvqgquapal5GRkbgq2sbqLZuJmNMeOkLAXEF7bqXVLXQ+74XWACcFIK6nLZVXW2g2hgTXkIaECKSApwOvOhzLFFEklsfA2cDn4SmQiDNuxfCxiGMMWEmmNNcnwTOAIaISAFwOxANoKoPeJd9DnhTVWt8XpoJLBCR1vr+qaqvB6vOLsUkQvJwm8lkjAk7QQsIVZ3bjWsexk2H9T22FZgWnKqOUPpY62IyxoSdvjAG0ffZVFdjTBiygOiO9HFQWwoH9oe6EmOM6TUWEN3ROtXVxiGMMWHEAqI7bKqrMSYMWUB0x+BckAgbhzDGhBULiO6IioWUHNuf2hgTViwguit9nHUxGWPCigVEd6WPdesxqYa6EmOM6RUWEN2VPg7qK6EmCCvGGmNMH2QB0V1tU11tHMIYEx4sILor3Vu0z8YhjDFhwgKiu1JGQkS0TXU1xoQNC4juioxy90NYF5MxJkxYQPRE+jgLCGNM2LCA6InWqa4tLaGuxBhjgs4CoifSx0LTAajaHepKjDEm6CwgesKmuhpjwogFRE/Yqq7GmDBiAdETycMhKt6NQxhjzABnAdETERGQNsa6mIwxYcECoqfSx1oXkzEmLAQtIETkIRHZKyKfdHD+DBGpEJGV3tfPfM6dKyIbRWSLiPwoWDUekfSxsH87NDeFuhJjjAmqYLYgHgbO7eKaJao63fu6E0BEIoE/AucBk4C5IjIpiHX2TPo4aGmEip2hrsQYY4IqaAGhqouBsiN46UnAFlXdqqoNwFPAJQEt7mi0TXW1gWpjzMAW6jGIWSKySkReE5HJ3rERwC6fawq8Y36JyA0iki8i+SUlPd+roaGphWc+3MVHO/d37wU21dUYEyZCGRAfAaNUdRrwB+CFI3kTVX1QVfNUNS8jI6PHr29R5devb+DPi7o5MykxA2IH2aquxpgBL2QBoaqVqlrtPX4ViBaRIUAhkONzabZ3LCjioiO54qQc3lpfTMH+2q5fIGJTXY0xYSFkASEiw0REvMcnebWUAh8C40VktIjEAFcALwWzlitnjgLgife7OfCcPs66mIwxA14wp7k+CSwDjhGRAhG5TkTmi8h875LLgE9EZBVwH3CFOk3ATcAbwHrgGVVdG6w6AYanxnPWpEye+mAndY3NXb8gfSxU7IKm+mCWZYwxIRUVrDdW1bldnL8fuL+Dc68Crwajro7Mm5XLG2uL+dfqIi47Ibvzi9PHgba4+yEyjumV+owxpreFehZTnzFrbDrjhibxyNLtqGrnF9uqrsaYMGAB4RER5s0axZrCClbuKu/84vQx7rvNZDLGDGAWED4+d3w2SbFRPLZsR+cXxg+GhHQbqDbGDGgWED6SYqP4wvEj+NfqIvZVdzEAnTbWupiMMQNatwJCRBJFJMJ7PEFELhaR6OCWFhpXzcqlobmFpz/c1fmF6eMsIIwxA1p3WxCLgTgRGQG8CVyFW4xvwBk3NIlTxw3h8eU7aGpu6fjC9DFub+qGmt4rzhhjelF3A0JUtRb4PPAnVf0iMLmL1/RbV88aRVFFHf9ev7fji9LHue+2u5wxZoDqdkCIyCzgSuAV71hkcEoKvTMnZjIiNZ5Hl23v+CKb6mqMGeC6GxDfBm4DFqjqWhEZA7wTtKpCLDJCuPLkkSz9tJTNxVX+L0qzqa7GmIGtWwGhqu+q6sWq+mtvsHqfqt4S5NpC6kt5OcRERfBoR1NeY5MgOctaEMaYAau7s5j+KSKDRCQR+ARYJyI/CG5poZWeFMtFU4fz/EcFVNU1+r/IproaYwaw7nYxTVLVSuBS4DVgNG4m04A2b/Yoahqaef6jDlYbTx9rN8sZYwas7gZEtHffw6XAS6raCHSxYFH/NzU7lWk5qTyyrIP1mdLHQu0+OFDe67UZY0ywdTcg/gJsBxKBxSIyCqgMVlF9ybxZo9haUsN/tpQefrJtqqt1MxljBp7uDlLfp6ojVPV8b8+GHcCcINfWJ5w/JYv0xBge8TfltW2qq90LYYwZeLo7SJ0iIneLSL739Ttca2LA63RL0sG5gFgLwhgzIHW3i+khoAq43PuqBP4RrKL6mg63JI2Og9QcG6g2xgxI3Q2Isap6u6pu9b7+BxgTzML6kk63JLWprsaYAaq7AXFARE5tfSIipwAHglNS3zRvVi77axv51+qiQ0+0rura1S50xhjTz3Q3IOYDfxSR7SKyHbeX9NeDVlUf1Lol6WPtB6vTx0J9BdT6meVkjDH9WHdnMa1S1WnAVGCqqs4APhvUyvqY1i1JVxW025K0daqrjUMYYwaYHu0op6qV3h3VAN/t7FoReUhE9orIJx2cv1JEVovIGhFZKiLTfM5t946vFJH8ntQYTK1bkj66dPvBg62L9tk4hDFmgDmaLUeli/MPA+d2cn4bcLqqTgF+DjzY7vwcVZ2uqnlHXmJg+d2SNHUURETZVFdjzIBzNAHR6aisqi4Gyjo5v1RV93tPlwPZR1FLrzlsS9LIKHc/hHUxGWMGmE4DQkSqRKTSz1cVMDyAdVyHWwSwlQJvisgKEbmhixpvaL2Br6SkJIAl+de6JekTvluSpo21u6mNMQNOpwGhqsmqOsjPV7KqRgWiABGZgwuIH/ocPlVVjwfOA74pIqd1UuODqpqnqnkZGRmBKKlLV88axW7fLUnTx7kuJpvqaowZQI6mi+moichU4G/AJaraNk9UVQu973uBBcBJoanQv8O2JE0fA421UFXU6euMMaY/CVlAiMhI4HngKlXd5HM8UUSSWx8DZ+M2KeozDtuStG2qqw1UG2MGjqAFhIg8CSwDjhGRAhG5TkTmi8h875KfAenAn9pNZ80E3hORVcAHwCuq+nqw6jxSrVuSPrZ8h8+qrjZQbYwZOAIyjuCPqs7t4vz1wPV+jm8Fph3+ir6ldUvS51YU8IOzx5McFWdTXY0xA0pIxyD6u7YtST8ucjfMWReTMWYAsYA4Cq1bkj66bDtqAWGMGWAsII7SvFmj+LSkhsLIEbB/G7Q0d/0iY4zpBywgjlLrlqRvFSdBcwNU7Ap1ScYYExAWEEepdUvSV3d7O7BaN5MxZoCwgAiAK2eOYpsOc08sIIwxA4QFRAAMT41nxsQJ1BBH077NoS7HGGMCwgIiQObNHs3WlmHs274u1KUMDNsWwxs/gZaWUFdiTNiygAiQWWPT2ReTg1oX09HbvwOe/gosux82vhrqaowJWxYQASIipI+cyNDmPazavjfU5fRfzY3w7FfdyrgpObDkt7ZKrjEhYgERQBMmTSdSlFcXLw91Kf3XW3dCYT5cdC+c9n3Y/TF8+naoqzImLFlABFDcsGMA2LZpNaWtW5Ka7tv0Biy9D/K+Csd9HqbNheThsOR3oa7MmLBkARFIaWMAyNHdPPWh3TDXIxWFsGA+ZB4H5/zKHYuKhVNugR3/gR3LQlufMWHIAiKQEtIgPo3ZqRX88Z0tLFxXHOqK+ofmJnjuOmiqhy8+DNFxB88dPw8ShlgrwpgQsIAItPSxfCatnPFDk7jhsXz+tGgLaoOsnVv0K9i5DC76PQwZf+i5mASY9Q3YshB2rwxFdcaELQuIQEsbS0zFdp7++iwunDqc37y+ke89s4q6RlvEz69P33atgxlfgamX+7/mxOshNsVaEcb0MguIQEsfB5UFxGk9910xne+dNYHnPy5k7l+Xs7eqrufvpwr7NsPG16Fkk+uOGSiq9sDzN0DGMXDe/3V8XVwKnPQ1WP8ylGzsvfqMCXNB21EubKW7gWr2b0MyJ3PzmeMZn5nEd55exaX3/4cHr87juBEpHb++odZN7dy1HHZ9ALvehwP7D56PjIH08TB0Igw9FjImuseDcyEiMqg/WkC1NMNz10N9Ncx72XUldebkb8DyP8GSu+Hzf+mdGo0JcxYQgZY+zn0v/RQyJwNw7nFZ5KQl8LVH8vniA8u4+/JpnDcly11XuduFwK4PYOdy2LMaWrxWwpAJcOyFkDPT9c2XbYOS9bB3g7v+k2cPfm5UnLt+qBcYGV6ApIyEiD7YUFz8W9i+BC75o6u3K4npcMK18P4DcMaPIG108Gs0JsxZQASaN9WV0i2HHJ48PIUXvjGTXz/8HMue+hVjFu9hQsNapKLAXRAVDyNOgNm3wMiTIftENyvK18iTD31eX+W6XPauh5INsHcdbFsCq58+eE10ouvCaR8cg0aASIB/+G7atgTevQumfgmmX9n9182+GT78q7tX4sJ7glefMQawgAi82GRIGgZln8KBcijI97qL3mdowQp+11gD0bBn72A+HjSNKWd9g+jck2HYVIiM7vlnZee5L18Hyr3A8AmOzQth5RMHr4lLcb9wT/1u73ZNVZe4rqW0MXDB3T0LqUFZLlA+fhxOu9U9N8YETVADQkQeAi4E9qrqcX7OC3AvcD5QC1yjqh955+YB/+1d+gtVfSSYtQZU+lhY/Qx8/ASgIBHuBrAZV0LOTDTnJBZ83MBv3tzIlI9T+OuUyWT2NBw6E5/qWhvtWxw1pV4X1Xo3e+jtX8CWt12ffurIwH1+R1paYMHX3ZjKV56F2KSev8cp34KPHnUL+Z3zy8DXaIxpI8Gcoy8ipwHVwKMdBMT5wM24gJgJ3KuqM0UkDcgH8gAFVgAnqOr+9u/hKy8vT/Pz8wP8UxyBlU/C2gXuL/ucma7ryM8vw4XrivnWUx+THBfFX6/OY2p2au/VqOq6ol75vguwC++GKZcF9zOX3A1v/Y/rHsr76pG/z/M3uBlN3/7EjU0YY46YiKxQ1Tx/54I6eqmqi4GyTi65BBceqqrLgVQRyQLOARaqapkXCguBc4NZa0BNnwtXPgOn3wpjTu/wL+WzJmXy3I2ziYqI4IsPLOPlVbt7r0YRmHYFzF/ixiieuw6e/zrUVQbn83Yudy2WyZ9zg81H49TvQmMtvP/nwNRmjPEr1NNbRgC+ixYVeMc6On4YEblBRPJFJL+kpCRohQbLxKxBvHjTKUzNTuHmJz/m7jc30tLSi3dep42Ga1+D038Ea56BB06Fne8H9jNqy9wS3qk5bpXWox0cH3osTLwI3n8Q6ioCU6Mx5jChDoijpqoPqmqequZlZGSEupwjMiQplsevn8kXT8jmvre38M1/fkRtQy/eEBcZBXNug2tfBxT+cS6886vA3JSnCi/cCNV73TpLcZ3cA9ITn/ke1FfAh38PzPsZYw4T6oAoBHJ8nmd7xzo6PmDFRkXym8um8pPzJ/L62j188YFl7C4/0LtFjJwJ89+DKZe7aaj/OM/de3E0lv0RNr0OZ/8Chs8ITJ3g3mvcf7n3b6gN3PsaY9qEOiBeAq4W52SgQlWLgDeAs0VksIgMBs72jg1oIsLXThvDQ/NOZEdpLRff/x8+3tnpuHzgxaW4WU1f+Lu7x+KBz8Cqp45sV7eCFfDv293NfjO/HvhaP/N9qN3nZjUZYwIuqAEhIk8Cy4BjRKRARK4TkfkiMt+75FVgK7AF+CvwDQBVLQN+Dnzofd3pHQsLc44dyoJvzCYhJpIvPbicBR8X9H4RUy6DG9+DYVPc1NTnrnP3V3TXgXJ49hq34c8l9wfnprxRs2DUKe7GuaaGwL+/MWEuqNNce1ufmeYaIGU1Ddz4+Are31bG108fw3f+awJx0b283lJLM7x3txuTGDQcPvcXyD2l89eowjNXwcbX3LhGzonBq2/Lv+HxL8BF98EJ84L3OcYMUCGb5mqOTlpiDI9dN5O5J43kL+9uZc5vF/HkBztpbG7pvSIiIuG0H8B1CyEiCh65EN76OTQ3dvyaD/7q7lM48/bghgPA2DMhazq8d8/AWunWmD7AAqKPi4mK4Fefn8I/r5/JsJQ4bnt+DWfd/S4vrizs3emw2Se4AezpX4Ylv4W/n+0WJGxv90p48ycw/hyYdVPw6xKB074P+7fBuheC/3nGhBHrYupHVJW31u/lt29uZMOeKo7JTOZ7Z0/grEmZSG8uvLf2BXj5W64Vcd6v3WY/Iu4muwdPh8Y6Fya9dZdzSwv8ebarYf5/+ubqtcb0UdbFNECICP81KZNXb/kMf5g7g8bmFm54bAWX/mkpSzaX9N7WppMvhRuXwojj4aWb4Jmr3c1w//o27N8Blz3Uu0tgRETAZ77rFiXc9Frvfa4xA5y1IPqxpuYWnv+okHvf2kxh+QFmjk7jB+ccQ15uWtcvDoSWZlj6B7eERkyCu6v5sz91XT69rbkJ7j8B4tPga2+HbilzY/oZa0EMUFGREVx+Yg5vf/907rhoEp+W1HDZA8u49h8f8ElhLyxBEREJp34brv+3m8464Ty3TlIoREbBKd+G3R/B1ndCU4MxA4y1IAaQ2oYmHlm6gwfe/ZSKA41cMCWL75w1gXFDj2BZ7Z5q/e8olH+5N9XDvdPdXhPXvhK6OszAsf5lePVWOP0HR7cCcR9mLYgwkRATxY1njGXJD+dwy2fHsWjjXs6+512+98wqdpUFeTkKkdB360TFuk2QdrznVo8NZ3UVsOIRN5mguv8tYhlyqrD0fnj6Kmiohn99B5b87shWFOjHrAUxgJVW1/PAu5/yyLIdqCpXnDiSmz47jsxBcaEuLXgaauD3U9weHFf+v1BX07tamuHTd2DVP2HDK9BU544PmQBXv+hudDRda26C138IH/4NJl7s9k1/5buw5v+5P0DO+nno/xgKoM5aEBYQYWBPRR1/eHszT3+4i8gI4ZrZucw/fSyDE2NCXVpwLP4tvP1z+PpiyJoW6mqCr3gdrHrS7WJYvQfiB8Nxl7l9SZrq4YnL3f7m816CwbmhrrZvq69yS9NvftPtXnjmHW6WXEsLvHar2xN9xlVu2fre3Ko3iCwgDAA7S2v5/VubWPBxIYkxUVw8fTgXTR3OSaPTiIwYOH8RUVcB90yBsWfA5QN0Ib+afbDmWddaKFrl7nIffzZMmwsTznHdba0KP4LHPw9R8a4lkTEhdHX3ZRWF8M8vuenSF/wO8tptbKUK7/wvLP6Na1l84W+H/nMOlZYWqCqCFL9b5nTJAsIcYnNxFX98ZwtvrC3mQGMzQ5NjuWBqFhdPG870nNTevekuWN76uesz/ub7bse8gaCpHja94VoLm9+ElibXQpr2Zbe4YuKQjl9bvBYevRS0Ba5+wS3CaA4qWg3/vBzqq+Hyh91S8h1Z9id44zYYMwe+9PiR7a0eKOW73H4r5TvhG8sgJrHHb2EBYfyqbWjirfV7eXnVbhZtLKGhuYXswfFcNM21LCZmJfffsKjZ58YiJl0Cn3sg1NUcOVXXAlj1JHzyLBzYD0nDYOrlrrWQOan777VvCzx6CTRUwVeed3umGxe6/+9a1zV35TOQObnr13z8hLtJdPjxbqwroZfuPWqlCiv/Ca//yIX+Of8Lx199RGMjFhCmS5V1jby5tpiXV+3mvS37aG5RxmYkurCYNpyxGSH8K+lIvX4bvP8XuOVjGDwq1NX0TEUhrH7a7cWxbyNExcGxF7jWwpgz3H0fR6J8JzxyMdSUwJefhtxTA1p2v/PBX93YwrApMPdpGJTV/deu/xc8ey2kj3OB25PXHo3qEjc7beMrbrn7S/90VGNLFhCmR0qr63l97R5eWrmbD7aXoQqTsgZx0bThXDg1i5y0hFCX2D2Vu+HeaW5Q8cK7A/OezY1QkA9bF8H299xf41Fxri86Kt777j2P9n0e53NdXMfn9q534wpb3wUUck52g82TPxe47Vori+CxS2H/dvjSEzC+k+6UgaqlGd78KSz/o7vB87K/H1H3DFsXwVNXQkK667pLGxPoSg+17iW3pE19NZz5Mzj5G0e99pgFhDlieyrqeGVNES+v2s3KXeUAHD8ylYumDeeCKVkM7etTZl/+Fqx8Er69GpKH9fz1qm7Qcusi90t7x3/cvHiJcMuMJw1100mb6qHxgPveVOfz5R2nB/+fpY503UfTrgjeL5yaffDY51wgXfYQTLo4OJ/TFzXUwHNfc3+Bz7wRzvnl0c1IKlgBT3wBImPgqgXd66LqqQPl8NoPYfVTbtzpc3+BoRMD8tYWECYgdpXV8vLq3by8qoj1RZWIwMmj07lo2nDOO25Y35w2W7YN/nACnOz9IuiO8l1eICyCbYuhZq87nj7Ode+MOcN1zcQP7t77qbqWR2tgNPkESWPdoccTMyD7pN5ZkfZAOTzxRShcAZf+GaZ9KfifGWpVxfDkl9zMr3PvCtxWuHs3uMBtrIErn4WckwLzvgCfvg0v3gRVe9w6Z6f9ACKjA/b2FhAm4LbsreLlVa5lsXVfDVERwinjhnDBlCzOnpxJakIfCovnb3D9xd/5xP9gYm0ZbF9ysJVQ5u1zkTj0YCCMOR1Ssnux6F5SXw1PXuG6yy685/CpnQNJ8To3U6m21LWajjkvsO+/f4fruqva42Y3jTvz6N6voQYW3u7uvRgywU22GHFCQEr1ZQFhgkZVWVdUyUurdvPqmiJ2lR0gKkKYNTbdC4thpIW6ZbF3PfzpZDjtVvjsT1yXz87lXgvhXbfJEQoxSa5lMOYMGH26a8L311lcPdF4AJ6ZB5vfcLNhZn0z1BUF3qdvu58xOsENzg+fHpzPqSp295yUbHT3SUy+9MjeZ9eHbi/4sk/dOMOZP3PjVkFgAWF6harySWElr6wp4tU1RewsqyUyQpg9Np3zjsvinMmZpCeF6Maip66EbUvcL4ady6G53t1cln3SwRbCiBMC2nTvV5oa4PmvuV355vzEdWMMlHBc8YhbSynjWDeNNdgtwQPl7oa7gg/gwt/3bK/0pgZ49y63he6gEW6G0ujTglUpYAFhQkBVWbu7kle9sNhe6sLi5DFpnHdcFuceN4whvRkWRavcNqnp41zrYMwZMGp2aG9y6muam+Clm90sqtm3wFl39u+QaGmBt+90v2zH/Rdc9g+IG9Q7n91QC89cBVv+7f45nvKtrl+z5xNYMB+K18D0r8C5v+qVekMWECJyLnAvEAn8TVXvanf+HmCO9zQBGKqqqd65ZmCNd26nqnY5zcICom9q7YZ6bc0eXl1TxNZ9NUQIzBydzvlTszh38jAyknshLFpabDvSrrS0wGs/cAvVnXg9nPd//fOfWeMB98t23QtwwrVw/m+P/N6RI9XU4LqJ1j4Pp34Hzrzdf+C2NMPS+9wyHnEpcNF9cOz5vVZmSAJCRCKBTcBZQAHwITBXVdd1cP3NwAxV/ar3vFpVe/TnnQVE36eqbNhTxatrinhlTRFbS2oQgZNy07jAC4s+P3V2oFOFhT9zv7SmfRku/kNgf7k2N0LxJ+5+ksIVbu2s2EHur+W4lIOP276nuOOtx6LjO2/Z1OyDJ+dCwYdw9s9h1k2hawm1NMOr34f8h+CEa+CCuw+dUlv6KbzwDdi1HCZe5LqkOlsyJQg6C4hgRupJwBZV3eoV8RRwCeA3IIC5wO1BrMf0ASLCxKxBTMwaxHfPmsCm4uq2MYufvbiW219ay4mj0jh/yjDOmjyM4Slx/Xe5j/5KxHWLxCbDO790Uzc//zeIOoLJBqpQsetgGBR86Lr7WpciTxzq7iWpq4T6CreaqrZ0/p4RUX4CxSdYNr3uZhJd/ohbaiWUIiJdKMSlwnt3uzD83INurCv/IXjzvyEi2h2benmf69ILZgviMuBcVb3ee34VMFNVb/Jz7ShgOZCtqs3esSZgJdAE3KWqL3TwOTcANwCMHDnyhB07dgT+hzG9YnNxVVtYbCquBiA1IZpjMpM5dlgyx2YN4phhyRyTmUxibC93F4SrpffDmz9xK8Ve/mjXM2nqq9zaUYX57gaygg8P3kcSFedu8hqRB9knQPaJkJJz6C9FVXcjYl2FFxqVPt8rDn4/7JzP+fhUN97Q19aa+s99sPCnMPazgMCnb7kF/y754xGvxBoIoepi6klA/BAXDjf7HBuhqoUiMgZ4GzhTVT/t7DOti2ng2LK3ivc272NjcRUb9lSxaU8VNQ3NbedHpiVwzLBkJg5L5phhgzg2K5nc9MSBtWx5X5H/EPzru24K8NynDg7stzS7KcSF+a6FUJAPJRtou2s8fZwXBt5X5nHhO0us1UePurv7o+JcK+3E60PeaghVF1MhkOPzPNs75s8VwCGTr1W10Pu+VUQWATOATgPCDBzjhiYzbmhy2/OWFqVg/wE27Klk4x4XGhv2VPLW+mJavN9HsVERjM9M4thhg1yLY5hrcfTKAPhAlvdViE50y0o/dqlbIK5whWspNNa4a+IHuzCYfKn7PuL43l/htD84/moYOhkS0/vF5k3BbEFE4Qapz8QFw4fAl1V1bbvrjgVeB0arV4yIDAZqVbVeRIYAy4BLOhrgbmUtiPBT19jMlr3VLjCKKttaHCVV9W3XpCfGcGyWC4zjRgxianYqo9MTibDWRs+sewmeu851Aw2b4loFrS2EtDEh/0vYHJmQtCBUtUlEbgLewE1zfUhV14rInUC+qr7kXXoF8JQemlQTgb+ISAsQgRuD6DQcTHiKi47kuBEpHDfi0JVOS6vrD2lpbNxTxRPv76Cu0Q2AJsdFMWVEClOyU5iWncrU7BRGpMbbgHhnJl0MY7ZAZCxE20yzcGA3ypmw0dTcwpaSalbvqmB1YTmrCypYX1RJY7P7fyA9MYYp2SlMzU5lmvfduqfMQBeqMQhj+pSoyAhvfGIQl5/ohsfqm5rZUFTF6sIKVu9yobF40+a2cY2slDimtoVGKlNGpJCSEOYDrSZsWECYsBYbFcm0nFSm5aTCyW7XuZr6JtYVVbLKC4zVBeW8sba47TW56QlM9bqlJmUNYnxmMkOSYqx7ygw4FhDGtJMYG8WJuWmcmHtwFk5FbSNrCitYVVDOmoIK8reX8dKq3W3nBydEMz4zmQmZSUzITGb8UPc4ZIsTGhMAFhDGdENKQjSnjh/CqeMPLoNQUuUGwjcVV7F5bxWbiqt5ceVuquqa2q5JT4xh3FAXGhMyk7wQSQ79EujGdIMFhDFHKCM5lozk2ENCQ1UprqxnU7ELji17q9lUXMULHxdSVX8wOIYkxbS1MlpDY0JmUt/aaMmEPQsIYwJIRBiWEsewlDhOm5DRdlxV2VNZx6biajZ74bGpuJpnVxQccod4emIM2WkJjExLIGdwPDltjxPISo0jOrIfrqxq+i0LCGN6gYiQlRJPVko8p7cLjt0Vda6bqriKbftq2FV2gFW7ynltTRFNLQenoUdGCFkpceQMTiAnLd4FR1oC2YNdiNhAuQk0CwhjQkhEGJEaz4jUeOYcM/SQc03NLRRV1LFrfy0FZQfYWVbLrv217Cqr5e0NJeyrrj/k+vjoSLIH+waHezx6SCIj0xOIjYrEmJ6wgDCmj4qKjCDH+2XP2MPPH2hopmB/rQuOslp27T/Q9nj51tJDuq5EYERqPKOHJJKbnsjoIe4rd0gi2YPjrevK+GUBYUw/FR8TyfjMZMZnJh92TlXZX9vIzrJatu+rYeu+Grbvq2F7aQ0vrCw8ZKZVVISQk5ZAbnoCo4ckMXpIArlegAxPibc1q8KYBYQxA5CIkJYYQ1piDNNzUg85p6qU1jSwfV8N27zQ2Lavhm37alm+tYwDjQdbHjFREYxKc4ExxmtxjEiNZ3hqHMNS4kmyfTkGNPu3a0yYERGGJMUyJCmWvNxDl+RWVfZW1bO1xAVHa4hs21fDu5tKaGg6dLe35NgosrywyBrkZm+1hkdWShxZKXEkx9nSJP2VBYQxpo2IkDkojsxBccwam37IueYWpajiAEUVde6rvPXxAfZU1LGhqJKS6nrar/+ZFBvFMC8sslIODY+slHiGpcQxKC7KZmD1QRYQxphuiYwQsge7abUdaWhqYW9VHXsq6thdUcee1kApr6Ooso6Ne0r8hkh8dKS7f8RrhWQOimPYoFiGeQEybFAcGcmxtmNgL7OAMMYETExURJch0tjcwt6qeorKD7C7oo7iijr2VHpfFXV8sK2MvVV1bcuwt4oQGJocR2aKC4+slHgXJCmxZA7yWiOD4oiPsem8gWIBYYzpVdGREW33fnSkpUUpq21gT4ULjT2VdRRXuq6t4so6tpbUsPTT0kNmY7VKjosiwxtjSU+KOeT7kLbn7nFSrHVtdcYCwhjT50REHBxIb79boK+a+iYXHl6IFFXUsbeyjn01DZRW17N5bzXLt5ayv7bR7+tjoyIODZLEGIYku+8ZybGkJ8YyJDmGrEHxDIoPvzCxgDDG9FuJsVGMzUhibEZSp9c1Nrewv6aBkup69lW78NhXXU9ptTtWWt1AcWUd63ZXUlpTf1j3FkBCTCRZKXEMT41neEo8Walxbd+zUtzU34SYgfUrdWD9NMYY40d0ZARDB8UxdFDXe2mrKpUHmrzgqKekut4Nupe7GVu7yw+wYU8VJVX1h702JT7aC5C4Q4IjK8WFyrCUOGKi+s9d6xYQxhjjQ0RISYgmJSGacUM7bpk0NLVQXFnHbm+6b2H5ATcNuNzN4Fqxcz/l7bq2RGBIUiyDE6JJjI0iKTaKxJgo73EkibHucWJM5MHzbV+RJMYcPNYbQWMBYYwxRyAmymetrA7UNjRRVOGFSHkdu70AKT/QQE19M1V1TRRX1lFT30x1fRM19U2HrODb6edHRrjQiI0iKyWO/zd/dqB+tDZBDQgRORe4F4gE/qaqd7U7fw3wf0Chd+h+Vf2bd24e8N/e8V+o6iPBrNUYYwItIaZ7YyStVJX6phZq6pvaQqO2ockLj2Zq6pvagqS6oYla71iwWhNBCwgRiQT+CJwFFAAfishLqrqu3aVPq+pN7V6bBtwO5AEKrPBeuz9Y9RpjTKiJCHHRkcRFR5LevUwJqmB2Yp0EbFHVraraADwFXNLN154DLFTVMi8UFgLnBqlOY4wxfgQzIEYAu3yeF3jH2vuCiKwWkWdFJKeHr0VEbhCRfBHJLykpCUTdxhhjCG5AdMfLQK6qTsW1Eno8zqCqD6pqnqrmZWRkdP0CY4wx3RLMgCgEcnyeZ3NwMBoAVS1V1dbJxH8DTujua40xxgRXMAPiQ2C8iIwWkRjgCuAl3wtEJMvn6cXAeu/xG8DZIjJYRAYDZ3vHjDHG9JKgzWJS1SYRuQn3iz0SeEhV14rInUC+qr4E3CIiFwNNQBlwjffaMhH5OS5kAO5U1bJg1WqMMeZwou0XZu/H8vLyND8/P9RlGGNMvyEiK1Q1z9+5UA9SG2OM6aMGVAtCREqAHUf48iHAvgCWE2h9vT6wGgOhr9cHfb/Gvl4f9K0aR6mq3ymgAyogjoaI5HfUzOoL+np9YDUGQl+vD/p+jX29PugfNYJ1MRljjOmABYQxxhi/LCAOejDUBXShr9cHVmMg9PX6oO/X2Nfrg/5Ro41BGGOM8c9aEMYYY/yygDDGGONX2AeEiJwrIhtFZIuI/CjU9bQnIjki8o6IrBORtSLyrVDX5I+IRIrIxyLyr1DX4o+IpHpLym8QkfUiMivUNbUnIt/x/h1/IiJPikhcH6jpIRHZKyKf+BxLE5GFIrLZ+z64j9X3f96/59UiskBEUkNVn1fPYTX6nPueiKiIDAlFbV0J64Dw2fXuPGASMFdEJoW2qsM0Ad9T1UnAycA3+2CNAN/i4GKLfdG9wOuqeiwwjT5Wq4iMAG4B8lT1ONz6ZVeEtioAHubwzbp+BLylquOBt7znofIwh9e3EDjO20ZgE3BbbxfVzsP42fDM2//mbGBnbxfUXWEdEBzdrne9QlWLVPUj73EV7heb382TQkVEsoELcEu29zkikgKcBvwdQFUbVLU8pEX5FwXEi0gUkADsDnE9qOpi3EKavi7h4N4tjwCX9mZNvvzVp6pvqmqT93Q5bruAkOngnyHAPcCtuG2V+6RwD4hu71zXF4hILjADeD/EpbT3e9x/6C0hrqMjo4ES4B9eN9jfRCQx1EX5UtVC4Le4vyaLgApVfTO0VXUoU1WLvMd7gMxQFtOFrwKvhbqI9kTkEqBQVVeFupbOhHtA9BsikgQ8B3xbVStDXU8rEbkQ2KuqK0JdSyeigOOBP6vqDKCG0HaLHMbrx78EF2bDgUQR+Upoq+qaunnyffIvYBH5Ca6L9olQ1+JLRBKAHwM/C3UtXQn3gOgXO9eJSDQuHJ5Q1edDXU87pwAXi8h2XBfdZ0Xk8dCWdJgCoEBVW1tez+ICoy/5L2CbqpaoaiPwPDA7xDV1pLh1sy/v+94Q13MYEbkGuBC4UvvezV5jcX8IrPL+v8kGPhKRYSGtyo9wD4gud70LNRERXN/5elW9O9T1tKeqt6lqtqrm4v75va2qfeovX1XdA+wSkWO8Q2cC60JYkj87gZNFJMH7d34mfWwg3cdLwDzv8TzgxRDWchgRORfX5XmxqtaGup72VHWNqg5V1Vzv/5sC4Hjvv9M+JawDwhvIat31bj3wjKquDW1VhzkFuAr3l/lK7+v8UBfVD90MPCEiq4HpwP+GtpxDea2bZ4GPgDW4/zdDvhyDiDwJLAOOEZECEbkOuAs4S0Q241o+d/Wx+u4HkoGF3v8vD4Sqvk5q7BdsqQ1jjDF+hXULwhhjTMcsIIwxxvhlAWGMMcYvCwhjjDF+WUAYY4zxywLCmB4QkWaf6cYrA7kCsIjk+lvx05hQiQp1Acb0MwdUdXqoizCmN1gLwpgAEJHtIvIbEVkjIh+IyDjveK6IvO3tTfCWiIz0jmd6exWs8r5al9WIFJG/evtCvCki8SH7oUzYs4Awpmfi23UxfcnnXIWqTsHdyft779gfgEe8vQmeAO7zjt8HvKuq03DrQrXewT8e+KOqTgbKgS8E9acxphN2J7UxPSAi1aqa5Of4duCzqrrVW1xxj6qmi8g+IEtVG73jRao6RERKgGxVrfd5j1xgobcRDyLyQyBaVX/RCz+aMYexFoQxgaMdPO6Jep/Hzdg4oQkhCwhjAudLPt+XeY+XcnDr0CuBJd7jt4AboW0/75TeKtKY7rK/TozpmXgRWenz/HVVbZ3qOthbLbYemOsduxm3k90PcLvaXesd/xbwoLeyZzMuLIowpg+xMQhjAsAbg8hT1X2hrsWYQLEuJmOMMX5ZC8IYY4xf1oIwxhjjlwWEMcYYvywgjDHG+GUBYYwxxi8LCGOMMX79f20mBTSelBweAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
        "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
        "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
        "layers, etc.).**"
      ],
      "metadata": {
        "id": "HnQo5CujdYZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [-1, 1]\n",
        "x_train = x_train / 127.5 - 1\n",
        "x_test = x_test / 127.5 - 1\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(32,32,3)),\n",
        "    Dense(300, activation='selu', kernel_initializer='lecun_normal'),\n",
        "    Dense(100, activation='selu', kernel_initializer='lecun_normal'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Nadam optimizer and categorical cross-entropy loss\n",
        "model.compile(optimizer=Nadam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback with a patience of 5 epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# Train the model for 100 epochs with batch size of 128 and early stopping\n",
        "history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-INOGMRTdhBv",
        "outputId": "dd2bddc4-a408-44e2-dbd9-9439e1e21ea8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 4s 6ms/step - loss: 1.7231 - accuracy: 0.4077 - val_loss: 1.5879 - val_accuracy: 0.4446\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.4557 - accuracy: 0.4918 - val_loss: 1.4730 - val_accuracy: 0.4846\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.3370 - accuracy: 0.5333 - val_loss: 1.4401 - val_accuracy: 0.4978\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.2483 - accuracy: 0.5615 - val_loss: 1.4135 - val_accuracy: 0.5095\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1717 - accuracy: 0.5885 - val_loss: 1.3911 - val_accuracy: 0.5186\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 1.1078 - accuracy: 0.6107 - val_loss: 1.3867 - val_accuracy: 0.5232\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.0553 - accuracy: 0.6280 - val_loss: 1.4478 - val_accuracy: 0.5098\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9957 - accuracy: 0.6486 - val_loss: 1.4133 - val_accuracy: 0.5271\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.9447 - accuracy: 0.6683 - val_loss: 1.4240 - val_accuracy: 0.5351\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.8924 - accuracy: 0.6832 - val_loss: 1.5099 - val_accuracy: 0.5183\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.8409 - accuracy: 0.7046 - val_loss: 1.4820 - val_accuracy: 0.5291\n",
            "Epoch 11: early stopping\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.4820 - accuracy: 0.5291\n",
            "Test accuracy: 0.5291000008583069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
        "see if you can achieve better accuracy using MC Dropout.**"
      ],
      "metadata": {
        "id": "Ij9Ovbp8gNQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, AlphaDropout\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.initializers import lecun_normal\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Convert pixel values to float32 and normalize to the range [-1, 1]\n",
        "x_train = x_train.astype('float32') / 127.5 - 1\n",
        "x_test = x_test.astype('float32') / 127.5 - 1\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Define the model architecture with dense layers and SELU activation\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(32,32,3)),\n",
        "    Dense(100, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    AlphaDropout(0.1),\n",
        "    Dense(100, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    AlphaDropout(0.1),\n",
        "    Dense(100, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    AlphaDropout(0.1),\n",
        "    Dense(100, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    AlphaDropout(0.1),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Nadam optimizer, categorical cross-entropy loss, and accuracy metric\n",
        "model.compile(optimizer=Nadam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback with a patience of 5 epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# Train the model for 100 epochs with batch size of 128 and early stopping\n",
        "history = model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "\n",
        "# Apply MC Dropout for inference\n",
        "T = 100  # Number of Monte Carlo samples to average\n",
        "y_preds = np.zeros((T, x_test.shape[0], 10))\n",
        "for i in range(T):\n",
        "    y_preds[i] = model.predict(x_test, batch_size=128)\n",
        "y_pred = np.mean(y_preds, axis=0)\n",
        "mc_test_acc = np.mean(np.equal(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
        "print(\"MC Test accuracy:\", mc_test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASCB2eJ5gS_4",
        "outputId": "4b4a00ab-e1f0-41fa-a74d-03cff23d82bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 5s 7ms/step - loss: 1.9295 - accuracy: 0.3191 - val_loss: 1.6950 - val_accuracy: 0.4302\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6564 - accuracy: 0.4063 - val_loss: 1.5616 - val_accuracy: 0.4683\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.5584 - accuracy: 0.4450 - val_loss: 1.5165 - val_accuracy: 0.4882\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.4966 - accuracy: 0.4652 - val_loss: 1.4980 - val_accuracy: 0.4930\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.4498 - accuracy: 0.4794 - val_loss: 1.4922 - val_accuracy: 0.5047\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.4044 - accuracy: 0.4979 - val_loss: 1.4445 - val_accuracy: 0.5087\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.3776 - accuracy: 0.5085 - val_loss: 1.4671 - val_accuracy: 0.5115\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.3417 - accuracy: 0.5184 - val_loss: 1.4528 - val_accuracy: 0.5243\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.3185 - accuracy: 0.5252 - val_loss: 1.4424 - val_accuracy: 0.5269\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.2948 - accuracy: 0.5353 - val_loss: 1.4883 - val_accuracy: 0.5210\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2738 - accuracy: 0.5464 - val_loss: 1.4398 - val_accuracy: 0.5343\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2544 - accuracy: 0.5520 - val_loss: 1.4269 - val_accuracy: 0.5409\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2388 - accuracy: 0.5554 - val_loss: 1.4587 - val_accuracy: 0.5407\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.2218 - accuracy: 0.5624 - val_loss: 1.4561 - val_accuracy: 0.5470\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.2034 - accuracy: 0.5694 - val_loss: 1.4868 - val_accuracy: 0.5374\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1904 - accuracy: 0.5742 - val_loss: 1.4437 - val_accuracy: 0.5359\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.1762 - accuracy: 0.5803 - val_loss: 1.4514 - val_accuracy: 0.5463\n",
            "Epoch 17: early stopping\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4514 - accuracy: 0.5463\n",
            "Test accuracy: 0.5462999939918518\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 4ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "MC Test accuracy: 0.5463\n"
          ]
        }
      ]
    }
  ]
}